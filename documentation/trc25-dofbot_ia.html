<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css"
    />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../styles/style-commun.css" />
  <link rel="stylesheet" href="../styles/style-code.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/srcery.min.css">
  <link rel="icon" type="image/x-icon" href="../logo/favicon.ico">
  <title>R√©silience Urbaine - B2MS CleanTech</title>
</head>
<body>
  <header class="header">
    <div class="section__container header__container_domaine">
      <h2>R√©silience Urbaine</h2>
    </div>
  </header>

  <div class="layout-container">
    <!-- Sidebar -->
    <div class="sidebar">
      <div class="sidebar-toggle" id="sidebarToggle">
        <i class="fas fa-bars"></i>
      </div>

      <nav class="main-nav">
        <a href=".././" class="nav-link"><i class="fas fa-home"></i> Accueil</a>
        <a href="./it" class="nav-link"><i class="fas fa-desktop"></i> IT</a>
        <a href="./electronique" class="nav-link"><i class="fas fa-microchip"></i> √âlectronique</a>
        <a href="./ROS" class="nav-link"><i class="fas fa-cogs"></i> M√©canique</a>
        <a href="./convoyeur" class="nav-link"></i><i class="fas fa-forward"></i> Convoyeur</a>

        <div class="nav-section">
          <a href="./trc25" style="text-decoration: none;"><button class="tab-button active"><i class="fas fa-robot"></i> TRC25</button> </a>       
          <div class="sub-links">
            <a class="subtab-button" href="./trc25-convoyeur">Convoyeur 2.0</a>
            <a class="subtab-button" href="./trc25-dofbot">Prise en main du Dofbot</a>
            <a class="subtab-button" href="./trc25-communication">Communication Dofbot - Convoyeur</a>
            <a class="subtab-button" href="./trc25-yolo">Mod√®le de d√©tection YOLO</a>
            <a class="subtab-button active" href="./trc25-dofbot_ia">D√©tection et tri - Dofbot</a>
            <nav class="project-subnav active" id="subnav-dofbot_ia">
              <a href="#1-dofbot_ia">1. D√©tection des dechets</a>
              <a href="#2-dofbot_ia">2. Contr√¥le et validation des mouvements</a>
              <a href="#3-dofbot_ia">3. Algorithme de tri des d√©chets</a>
              <a href="#4-dofbot_ia">4. Tests et Demonstration</a>
              <a href="#5-dofbot_ia">5. Suivi et gestion de l‚Äôhistorique</a>
            </nav>
            <a class="subtab-button" href="./trc25-ros">ROS</a>
            <a class="subtab-button" href="./trc25-meca">Syst√®me de ramassage</a>
          </div>
        </div>

      </nav>
    </div>

    <!-- Contenu principal -->
    <main class="main-content">
      <section class="tab-content active">
        <div class="subtab-content active">
          <h2 id="dofbot_ia" class="projet-titre">D√©tection et tri - Dofbot</h2>
            <p>
              Cette section est consacr√©e √† la d√©tection et au tri automatique des d√©chets √† l‚Äôaide du bras robotis√© Dofbot, 
              √©l√©ment central du syst√®me intelligent de tri d√©velopp√© dans le cadre du projet TRC25.
              Apr√®s l‚Äô√©tude du mod√®le de d√©tection bas√© sur YOLO, l‚Äôobjectif est d√©sormais d'implementer le mod√®le et d‚Äôassurer une prise de d√©cision autonome.
            </p>
            <ul>
              Cette section d√©taille :
              <li>l‚Äôint√©gration du mod√®le YOLO dans le Dofbot,</li>
              <li>le processus de d√©tection et de validation des d√©chets,</li>
              <li>la logique de tri adopt√©e,</li>
              <li>ainsi que la commande et la synchronisation des mouvements du bras robotis√©.</li>
            </ul>

          <h3 id="1-dofbot_ia">1. D√©tection des dechets</h3>
            <h4>a. Choix du mod√®le et contraintes d‚Äôint√©gration</h4>
            <p>
              Lors de la section <a href="./trc25-yolo.html"><strong>Mod√®le de d√©tection YOLO</strong></a>, une √©tude comparative a √©t√© men√©e entre <strong>YOLOv5</strong> et <strong>YOLOv8</strong>. 
              Les r√©sultats obtenus ont montr√© des performances similaires.
            </p>
            <p>
              Cependant, ces deux mod√®les, entra√Æn√©s avec la biblioth√®que <strong>Ultralytics</strong>, ont pos√© des probl√®mes de compatibilit√© lors de leur int√©gration dans le Dofbot. 
              En effet, le Dofbot utilise <strong>Python 3.6.9</strong> et dispose de <strong>versions ant√©rieures de plusieurs biblioth√®ques</strong> telles que numpy, matplotlib ou torch, 
              ce qui rend la mise √† jour complexe et susceptible d‚Äôintroduire d‚Äôautres incompatibilit√©s.
            </p>
            <p>
              Afin de garantir une <strong>int√©gration stable, ma√Ætris√©e et exploitable</strong>, il a √©t√© n√©cessaire de r√©entra√Æner le mod√®le de d√©tection en utilisant <strong>YOLOv5 bas√© sur PyTorch</strong>.
            </p>

            <h4>b. Entrainement du mod√®le YOLOv5 Pytorch</h4>
              <p>
                L‚Äôentra√Ænement du mod√®le YOLOv5-PyTorch pour le Dofbot suit globalement la m√™me d√©marche que celle utilis√©e avec Ultralytics, avec quelques points sp√©cifiques √† prendre en compte pour garantir la compatibilit√©.
              </p>
              <ul>
                <li>
                  <strong>T√©l√©chargement des images labellis√©es :</strong>
                  Il est essentiel de s‚Äôassurer que les annotations et les images sont compatibles avec YOLOv5-PyTorch et non avec la version Ultralytics utilis√©e pr√©c√©demment. 
                  Il faut alors utiliser la version <strong>YOLOv5-Pytorch</strong>.
                <li>
                  <strong>La version utilis√©e</strong>:
                  Les mod√®les de detection YOLO ne cesse d'√™tre mise √† jour, il faut donc faire attention √† la version utilis√©e.
                </li>
                <li>
                  <strong>les fichiers yolov5.yaml</strong>: nc : 3 #nombre de classes
                  <br>
                  Ces fichiers d√©crivent la structure du r√©seau de neurones YOLOv5, c‚Äôest-√†-dire :
                  <ul>
                    <li>le type de backbone (extraction des features)</li>
                    <li>le neck (FPN / PANet pour la fusion multi-√©chelle)</li>
                    <li>le head (d√©tection : bounding boxes, classes, scores)</li>
                    <li>les param√®tres de profondeur et de largeur du r√©seau</li>
                  </ul>
                </li>
              </ul>

            <p>
              L'architecture du dossier est comme suit:
            </p>
            <div class="arbo">
              <ul>
                  <li class="folder">garbage
                    <ul>
                      <li class="folder">train
                        <ul>
                          <li class="folder">images
                            <ul>
                              <li class="img">image1.jpg</li>
                              <li class="img">image2.jpg</li>
                              <li class="img">image3.jpg</li>
                              <li class="img">...</li>
                            </ul>
                          <li class="folder">labels
                            <ul>
                              <li class="txt">image1.txt</li>
                              <li class="txt">image2.txt</li>
                              <li class="txt">image3.txt</li>
                              <li class="txt">...</li>
                            </ul>
                        <li class="txt" style="display: block">garbage.yaml</li>      
                        </ul>
                      </li>   
                    </ul>
                  </li>  
                  <li class="folder">models
                    <ul>
                      <li class="txt">__init__.py</li>
                      <li class="txt">common.py</li>                    
                      <li class="txt">experimental.py</li>                    
                      <li class="txt">export.py</li>                    
                      <li class="txt">yolo.py</li>     
                      <li class="txt">yolov5s.yaml</li>                    
                      <li class="txt">...</li>                  
                    </ul>
                  </li>
                  <li class="folder">utils
                    <ul>
                      <li class="folder">google_app_engine</li>                  
                      <li class="txt">__init__.py</li>
                      <li class="txt">activations.py</li>                    
                      <li class="txt">datasets.py</li>
                      <li class="txt">evolve.sh</li>                    
                      <li class="txt">general.py</li>                    
                      <li class="txt">google_utils.py</li>     
                      <li class="txt">torch_utils.py</li>                    
                    </ul>
                  </li>
                  <li class="folder">weights
                    <ul>
                      <li class="txt">download_weights.sh</li>
                      <li class="txt">yolov5.pt</li>                    
                    </ul>
                  </li>
                  <li class="txt">detect.py</li>
                  <li class="txt">requirements.txt</li> 
                  <li class="txt">test_dataset.py</li>                    
                  <li class="txt">test.py</li>                    
                  <li class="txt">train.py</li>
                </ul>
            </div>

            <p>
              <strong>NB:</strong>
              Il est primodial de v√©rifier les annotations (format YOLOv5-PyTorch : classe x_center y_center width height).
            </p>
            <details class="code-container">
                <summary>test_dataset.py</summary>
                <div class="btn-bar">
                  <button onclick="copierCode('test_dataset-py', this)">üìã</button>
                  <button onclick="telechargerCode('test_dataset-py', 'test_dataset.py', 'text/py', this)">ü°á</button>
                </div>
                <pre><code class="hljs" id="test_dataset-py">
import os
import glob
import numpy as np

img_dir = "./garbage/train/labels/"
txt_files = glob.glob(os.path.join(img_dir, "*.txt"))

for f in txt_files:
    with open(f, "r") as file:
        for line in file.readlines():
            parts = line.strip().split()
            if len(parts) != 5:
                print(f"Erreur dans {f}: {line.strip()}")
            try:
                np.array([float(x) for x in parts])
            except:
                print(f"Valeur invalide dans {f}: {line.strip()}")
                </code></pre>
            </details>

            <p>
              Pour entra√Æner le mod√®le YOLOv5-PyTorch compatible avec le Dofbot, 
              il est n√©cessaire de pr√©parer un environnement Python sp√©cifique et de lancer le script d‚Äôentra√Ænement. 
              Les commandes suivantes permettent de reproduire l‚Äôenvironnement sur un ordinateur et d'entrainer le mod√®le:
            </p>
                <div class="code-container">
                  <div class="btn-bar">
                    <button onclick="copierCode('training_yolov5', this)">üìã</button>
                  </div>
                  <pre><code class="hljs" id="training_yolov5">
# 1. Cr√©ation de l'environnement conda avec Python 3.6.9
conda create --name b2ms-env python=3.6.9 -y

# 2. Activation de l'environnement
conda activate b2ms-env

# 3. Installation des d√©pendances n√©cessaires
pip install -r requirements.txt

# 4. Lancement de l'entra√Ænement du mod√®le
python train.py --device 0 --workers 0 --epochs 300
                  </code></pre>
                </div>   

                <p>
                  Cette s√©quence garantit que le mod√®le est entra√Æn√© dans un environnement compatible avec le Dofbot, 
                  tout en respectant les versions de Python et des biblioth√®ques existantes.
                </p>

                <p class="txt_center">
                  <a href="https://github.com/TekBot-Robotics-Challenge/2025-Team-B2MS_CleanTech-Docs/raw/refs/heads/main/TRC25/b2ms_YOLO/yolov5_pytorch_training.zip" 
                  target="_blank" class="download-link">T√©l√©charger yolov5_pytorch_training.zip</a>
                </p>

          <h4>c. D√©ploiement du mod√®le</h4>
              <p>
                Une fois l‚Äôentra√Ænement du mod√®le YOLOv5-PyTorch termin√©, le mod√®le obtenu doit √™tre charg√© dans Dofbot afin de permettre la d√©tection des d√©chets:
                <strong><em>b2ms/yolov5-pytorch/model4.pt</em></strong>
              </p>

              <p>
                Il est alors possible de lancer la d√©tection √† l‚Äôaide de <strong>detect.py</strong> 
                en sp√©cifiant diff√©rents param√®tres, notamment : <em>--source</em>, <em>--weights</em>, <em>--output</em>, etc.
              </p>

              <p>
                Afin de tester et valider le mod√®le sur le Dofbot, deux approches compl√©mentaires ont √©t√© adopt√©es.
              </p>

              <h5>&#10022; D√©tection sur images statiques</h5>
                <p>
                  Dans un premier temps, le mod√®le a √©t√© test√© sur un ensemble d‚Äôimages statiques.
                  Cette √©tape permet de :
                  <ul>
                    <li>
                      v√©rifier le bon chargement du mod√®le,
                    </li>
                    <li>
                      valider la compatibilit√© entre le mod√®le et le Dofbot,
                    </li>
                    <li>
                      analyser visuellement la pr√©cision des d√©tections (classes, bounding boxes, probabilit√©s),
                    </li>
                    <li>
                      corriger d‚Äô√©ventuelles erreurs avant le passage au temps r√©el.
                    </li>
                  </ul>
                </p>

                <p>
                  Les images sont trait√©es une par une, 
                  et les r√©sultats de d√©tection sont affich√©s directement sur les images, facilitant ainsi l‚Äô√©valuation qualitative du mod√®le.
                </p>

            <figure>
              <video controls width=auto>
                <source src="../videos/TRC25/Dofbot_IA/yolov5_statique.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>
            </figure>

            <h5>&#10022; D√©tection en temps r√©el avec la cam√©ra</h5>
                <p>
                  Une fois les tests sur images valid√©s, le mod√®le est int√©gr√© √† un flux vid√©o issu de la cam√©ra du Dofbot.
                  Cette seconde approche permet de :
                  <ul>
                    <li>
                      √©valuer les performances du mod√®le en conditions r√©elles,
                    </li>
                    <li>
                      mesurer le temps de traitement par image,
                    </li>
                    <li>
                      tester la stabilit√© de la d√©tection sur plusieurs frames cons√©cutives,
                    </li>
                  </ul>
                </p>

                <p>
                  Cette √©tape constitue une transition essentielle entre la validation du mod√®le et son exploitation op√©rationnelle dans le syst√®me de tri automatique.
                </p>

                <p>
                  Lors du lancement de la d√©tection, il est possible qu‚Äôune erreur apparaisse. 
                  Celle-ci est g√©n√©ralement due au fait que la cam√©ra est d√©j√† utilis√©e par un autre processus, emp√™chant ainsi son acc√®s par le script de d√©tection du Dofbot.
                  <br>
                  Dans ce cas, il suffit d‚Äôarr√™ter le processus en cours qui utilise la cam√©ra afin de lib√©rer la ressource.
                  La proc√©dure √† suivre est illustr√©e dans la vid√©o suivante, qui montre comment identifier et interrompre le processus concern√©.
                </p>
              <video controls width=auto>
                <source src="../videos/TRC25/Dofbot_IA/yolov5_camera_erreur.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>

              <p>
                Ces commandes ont √©t√© int√©gr√©es dans un <strong>script de lancement .sh</strong>, permettant d‚Äôautomatiser l‚Äôinitialisation de l‚Äôenvironnement et le d√©marrage de la d√©tection.
                Cette approche facilite l‚Äôutilisation du syst√®me, limite les erreurs de manipulation et garantit une ex√©cution coh√©rente √† chaque lancement du Dofbot. 
              </p>

              <details class="code-container">
                <summary>launch_detection.sh</summary>
                <div class="btn-bar">
                  <button onclick="copierCode('launch_detection-sh', this)">üìã</button>
                  <button onclick="telechargerCode('launch_detection-sh', 'launch_detection.sh', 'text/sh', this)">ü°á</button>
                </div>
                <pre><code class="hljs" id="launch_detection-sh">
#!/bin/bash

DEVICE="/dev/video0"

SCRIPT="/home/jetson/b2ms/yolov5-pytorch/detect.py"
WEIGHTS="/home/jetson/b2ms/yolov5-pytorch/model4.pt"

echo "V√©rification de l'utilisation de $DEVICE..."

# R√©cup√©rer les PID qui utilisent /dev/video0 (la camera)
PIDS=$(fuser $DEVICE 2>/dev/null)

if [ -n "$PIDS" ]; then
    echo "Processus utilisant $DEVICE : $PIDS"
    for PID in $PIDS; do
        echo "Killing PID $PID"
        sudo kill -9 "$PID"
    done
    sleep 1
else
    echo "Aucun processus n'utilise $DEVICE"
fi

echo "Lancement de la d√©tection..."
python "$SCRIPT" --source 0 --weights "$WEIGHTS"
                </code></pre>
              </details>

              <video controls width=60%>
                <source src="../videos/TRC25/Dofbot_IA/yolov5_camera.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>

            <p class="txt_center">
              <strong>NB:</strong>
              Le dossier <strong><em>yolov5-pytorch</em></strong> est fourni dans l‚Äôarchive <strong><em>b2ms_Dofbot_IA.zip</em></strong>, disponible √† la fin de cette section.
            </p>

            <h5>&#10022; Script d‚Äôidentification des d√©chets</h5>
              <p>
                Pour permettre au Dofbot d‚Äôidentifier les d√©chets d√©tect√©s par le mod√®le pr√©-entrain√©, un script Python d√©di√© a √©t√© cr√©√© : <strong><em>garbage_identify.py</em></strong>.
              </p>
              <ul>
                Ce script a pour r√¥le principal de :
                <li>d√©tecter les types de d√©chets dans l‚Äôimage,</li>
                <li>renvoyer les coordonn√©es de l‚Äôobjet dans le rep√®re de l‚Äôimage.</li>
              </ul>

              <details class="code-container">
                <summary>garbage_identify.py</summary>
                <div class="btn-bar">
                  <button onclick="copierCode('garbage_identify-py', this)">üìã</button>
                  <button onclick="telechargerCode('garbage_identify-py', 'garbage_identify.py', 'text/py', this)">ü°á</button>
                </div>
                <pre><code class="hljs" id="garbage_identify-py">
#!/usr/bin/env python3
# coding: utf-8

import time
import torch
import cv2 as cv
import numpy as np
from numpy import random

from utils.torch_utils import select_device
from models.experimental import attempt_load
from utils.general import (
    non_max_suppression,
    scale_coords,
    xyxy2xywh,
    plot_one_box
)

model_path = '/home/jetson/b2ms/yolov5-pytorch/model4.pt'

# Initialize device
device = select_device()

# Load YOLO model
model = attempt_load(model_path, map_location=device)

# Get names and colors
names = model.module.names if hasattr(model, 'module') else model.names
# Get the color value randomly
colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]


class garbage_identify:
    """
    YOLO-based garbage identification class
    ROLE:
    - Detect garbage type
    - Return {class_name: (x, y)} in image coordinates
    - DOES NOT control the robot arm
    """

    def __init__(self):
        self.frame = None
        self.garbage_index = 0  # used to delay detection at startup
        self.model_ready = False
        
    def warmup(self):
        """
        Warmup YOLO model at startup
        """
        dummy = np.zeros((480, 640, 3), dtype=np.uint8)
        for _ in range(5):
            self.garbage_run(dummy)

        self.model_ready = True

    def garbage_run(self, image):
        """
        Run garbage detection
        :param image: Original camera frame
        :return: frame (with drawings), msg {name: (x, y)}
        """
        self.frame = cv.resize(image, (640, 480))
        msg = {}

        if not self.model_ready:
            cv.putText(
                self.frame,
                "MODEL LOADING...",
                (150, 50),
                cv.FONT_HERSHEY_SIMPLEX,
                1,
                (0, 0, 255),
                2
            )
            return self.frame, msg

        try:
            msg = self.get_pos()
        except Exception as e:
            print("YOLO detection error:", e)

        return self.frame, msg

    def get_pos(self):
        """
        Perform YOLO inference and extract object position
        :return: dict {class_name: (x, y)}
        """
        msg = {}

        # Copy frame to avoid modifying original
        img = self.frame.copy()

        # Preprocess
        img = np.transpose(img, (2, 0, 1))  # HWC ‚Üí CHW
        img = torch.from_numpy(img).to(device)
        img = img.float() / 255.0

        if img.ndimension() == 3:
            img = img.unsqueeze(0)

        # Inference
        pred = model(img)[0]

        # Apply Non-Max Suppression
        pred = non_max_suppression(pred, 0.4, 0.5)

        gn = torch.tensor(self.frame.shape)[[1, 0, 1, 0]]

        if pred != [None]:
            for det in pred:
                if det is None:
                    continue

                # Rescale boxes
                det[:, :4] = scale_coords(
                    img.shape[2:], det[:, :4], self.frame.shape
                ).round()

                for *xyxy, conf, cls in reversed(det):
                    class_id = int(cls)
                    name = names[class_id]

                    # Bounding box center
                    xywh = (
                        xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn
                    ).view(-1).tolist()

                    cx = int(xywh[0] * 640)
                    cy = int(xywh[1] * 480)

                    # Draw center point
                    cv.circle(self.frame, (cx, cy), 5, (0, 0, 255), -1)

                    # Draw bounding box
                    label = f"{name} {conf:.2f}"
                    plot_one_box(
                        xyxy,
                        self.frame,
                        label=label,
                        color=colors[class_id],
                        line_thickness=2
                    )

                    # Convert pixel coordinates to normalized robot coordinates
                    x = round((cx - 320) / 4000, 5)
                    y = round(((480 - cy) / 3000) * 0.8 + 0.19, 5)

                    msg[name] = (x, y)

        return msg
                </code></pre>
              </details>
          
          <h3 id="2-dofbot_ia">2. Contr√¥le et validation des mouvements</h3>
            <p>
              Avant de contr√¥ler le bras robotis√©, il est indispensable de positionner correctement le Dofbot dans son environnement de travail. 
              Ceci permet d‚Äôanticiper les contraintes m√©caniques, d‚Äô√©viter les collisions potentielles et de s‚Äôassurer que les mouvements du bras seront effectifs et reproductibles dans les conditions r√©elles d‚Äôutilisation.
            </p>
            <div class="image-container">
              <img src="../images/TRC25/Dofbot_IA/environnement.webp" alt="environnement" class="image-mean"/>
            </div>
            <p>
              Une fois l‚Äôenvironnement d√©fini, les mouvements du bras Dofbot sont contr√¥l√©s et test√©s ind√©pendamment de la d√©tection. 
              Chaque mouvement souhait√© est programm√© puis valid√©, notamment :
              <ul>
                <li>
                  La position initiale : permet au bras de se placer correctement pour d√©tecter les objets sur le convoyeur.
                </li>
                <li>
                  La prise des objets : inclut le d√©placement vers l‚Äôobjet, l‚Äôouverture/fermeture de la pince et la s√©curisation de la pr√©hension.
                </li>
                <li>
                  Le d√©p√¥t selon sa cat√©gorie : transport de l‚Äôobjet vers la zone de tri correspondante et retour √† la position initiale.
                </li>
              </ul>
            </p>
            <p>
              Cette s√©quence permet de tester et v√©rifier la pr√©cision, la stabilit√© et la fiabilit√© de chaque mouvement avant leur int√©gration dans l‚Äôalgorithme de tri automatique, garantissant ainsi un fonctionnement s√ªr et efficace du Dofbot.
            </p>

            <p>
              <strong>Remarque:</strong><br>
              Les servomoteurs du bras Dofbot peuvent √™tre contr√¥l√©s individuellement ou simultan√©ment. 
              Cependant, √©tant donn√© que les servomoteurs 3 et 6 sont interd√©pendants (le servomoteur 6 influence directement le mouvement du 3), 
              il est pr√©f√©rable de s√©parer les commandes autant que possible. D'ailleurs, la commande sur le 3 seul ne fonctionne pas.
              <br>
              Le contr√¥le individuel est effectu√© par la commande: <strong>Arm.Arm_serial_servo_write(id, angle, mvt_time)</strong>
              <ul>
                Cette approche permet de :
                <li>
                  limiter les conflits entre mouvements,
                </li>
                <li>
                  garantir une pr√©cision accrue lors de la pr√©hension et du d√©p√¥t,
                </li>
                <li>
                  faciliter la programmation et le d√©bogage des s√©quences de mouvement.
                </li>
              </ul>
            </p>

            <p>
              En pratique, la s√©quence de tri est donc con√ßue pour commander chaque servomoteur individuellement, 
              tout en synchronisant les mouvements essentiels pour assurer un fonctionnement fluide et s√ªr.
            </p>

            <video controls width=80%>
              <source src="../videos/TRC25/Dofbot_IA/ctrl_servo.mp4" type="video/mp4">
              Votre navigateur ne supporte pas la vid√©o HTML5.
            </video>

            <p>
              Apr√®s validation des mouvements √† l‚Äôaide de Jupyter Notebook, 
              <strong>un script Python</strong> d√©di√© est cr√©√© afin d‚Äô√™tre utilis√© directement dans le processus de tri automatique.
              Ce script regroupe l‚Äôensemble des s√©quences de mouvement valid√©es 
              et permet une int√©gration simple et efficace avec l‚Äôalgorithme de d√©tection et de d√©cision du syst√®me de tri.
            </p>
              <details class="code-container">
                <summary>garbage_grab.py</summary>
                <div class="btn-bar">
                  <button onclick="copierCode('garbage_grab-py', this)">üìã</button>
                  <button onclick="telechargerCode('garbage_grab-py', 'garbage_grab.py', 'text/py', this)">ü°á</button>
                </div>
                <pre><code class="hljs" id="garbage_grab-py">
#!/usr/bin/env python
# coding: utf-8
import Arm_Lib
from time import sleep

class garbage_grap_move:
    def __init__(self):
        # set move status
        self.move_status = True
        self.arm = Arm_Lib.Arm_Device()
        # Clamping jaw tightening angle
        self.grap_joint = 134
        self.mvt_time = 550
        self.turn_dangereux = 140
        self.turn_menagers = 20
        self.arm.Arm_RGB_set(0, 0, 0) #RGB off
        sleep(.5)


    def mvt_pos_initial(self):
        '''
        initial position to detect object
        '''
        self.arm.Arm_RGB_set(0, 0, 0) #RGB off
        sleep(.1)
        
        self.arm.Arm_serial_servo_write(6, 120, self.mvt_time + 400)  # servo 3, 6
        sleep(0.2)
        self.arm.Arm_serial_servo_write(1, 90, self.mvt_time + 400)   # servo 1
        sleep(0.2)
        self.arm.Arm_serial_servo_write(2, 90, self.mvt_time + 400)   # servo 2
        sleep(0.2)
        self.arm.Arm_serial_servo_write(4, 50, self.mvt_time + 400)   # servo 4
        sleep(0.2)
        self.arm.Arm_serial_servo_write(5, 90, self.mvt_time + 400)   # servo 5
        sleep(0.2)
        self.arm.Arm_serial_servo_write(6, 175, self.mvt_time + 400)  # servo 3, 6
        sleep(1)
        
    def mvt_pos_object(self):
        '''
        position on object
        '''
        self.arm.Arm_serial_servo_write(5, 270, self.mvt_time)   # servo 5
        sleep(0.2)
        self.arm.Arm_serial_servo_write(4, 70, self.mvt_time)   # servo 4
        sleep(0.2)
        self.arm.Arm_serial_servo_write(6, 100, self.mvt_time)   # servo 6
        sleep(0.5)
        self.arm.Arm_serial_servo_write(2, 16, 3*self.mvt_time)   # servo 2
        sleep(2)
    
    def mvt_take_object(self):
        '''
        position on object
        '''
        self.arm.Arm_serial_servo_write(2, 22, 200)   # servo 2
        sleep(0.2)

        self.arm.Arm_serial_servo_write(6, 110, self.mvt_time + 200)   # servo 6
        sleep(0.4)

        self.arm.Arm_serial_servo_write(2, 28, self.mvt_time + 200)   # servo 2
        sleep(0.2)

        self.arm.Arm_serial_servo_write(6, 125, self.mvt_time + 200)   # servo 6
        sleep(0.4)

        self.arm.Arm_serial_servo_write(2, 35, self.mvt_time + 200)   # servo 2
        sleep(0.8)

        self.arm.Arm_serial_servo_write(6, 134, self.mvt_time + 200)   # servo 6
        sleep(0.3)

        self.arm.Arm_serial_servo_write(2, 44, self.mvt_time + 200)   # servo 2
        sleep(1)
        
    def mvt_up(self):
        '''
        move up object
        '''
        self.arm.Arm_serial_servo_write(2, 70, self.mvt_time)   # servo 2
        sleep(0.5)
    
    def grab_object(self):
        self.mvt_pos_object()
        self.mvt_take_object()
        self.mvt_up()

        
    def mvt_dangereux(self):
        '''
        recycle hazardous waste
        '''
        self.arm.Arm_RGB_set(50, 0, 0) #RGB red
        sleep(.5)
        
        self.grab_object()
        
        self.arm.Arm_serial_servo_write(1, self.turn_dangereux, self.mvt_time)  # servo 4
        sleep(0.2)
        self.arm.Arm_serial_servo_write(4, 30, self.mvt_time)   # servo 4
        sleep(0.4)
        self.arm.Arm_serial_servo_write(6, 120, self.mvt_time)   # servo 6
        sleep(0.8)
        
        
    def mvt_menagers(self):
        '''
        recycle kitchen waste
        '''
        self.arm.Arm_RGB_set(0, 0, 50) #RGB blue
        sleep(.5)
        
        self.grab_object()
        
        self.arm.Arm_serial_servo_write(1, self.turn_menagers, self.mvt_time)  # servo 4
        sleep(0.2)
        self.arm.Arm_serial_servo_write(4, 30, self.mvt_time)   # servo 4
        sleep(0.4)
        self.arm.Arm_serial_servo_write(6, 120, self.mvt_time)   # servo 6
        sleep(0.8)
        
    def mvt_recyclables(self):
        '''
        recycle recyclable waste
        '''
        self.arm.Arm_RGB_set(0, 50, 0) #RGB green
        sleep(.5)
        
        self.grab_object()
        
        self.arm.Arm_serial_servo_write(2, 175, self.mvt_time*2)    # servo 2
        sleep(0.8)
        self.arm.Arm_serial_servo_write(4, 120, self.mvt_time)  # servo 4
        sleep(0.6)
        self.arm.Arm_serial_servo_write(6, 120, self.mvt_time)   # servo 6
        sleep(0.8)

    def mvt_others(self):
        '''
        recycle others waste: not identified
        '''
        self.arm.Arm_RGB_set(255, 165, 0) #RGB orange
        sleep(.5)
        
        self.grab_object()
        
        self.arm.Arm_serial_servo_write(2, 175, self.mvt_time)    # servo 2
        sleep(1)
        self.arm.Arm_serial_servo_write(4, 120, self.mvt_time)  # servo 4
        sleep(2)
        self.arm.Arm_serial_servo_write(6, 120, self.mvt_time)   # servo 6
        sleep(2)
        
    def arm_run(self, name):
        '''
        Manipulator movement function
        :param name: Identified spam names
        '''
        # Hazardous waste - red
        if name == "dangereux" and self.move_status == True:
            # It is set here. You can only run down after this operation
            self.move_status = False
            # print("Hazardous waste")
            self.mvt_dangereux()
            self.move_status = True
            
        # Kitchen waste - blue
        if name == "menagers" and self.move_status == True:
            self.move_status = False
            # print("Kitchen waste")
            self.mvt_menagers()
            self.move_status = True
            
        # Recyclable waste - green
        if name == "recyclables" and self.move_status == True:
            self.move_status = False
            # print("Recyclable waste")
            self.mvt_recyclables()
            self.move_status = True
        
        self.mvt_pos_initial()
                </code></pre>
              </details>

          <h3 id="3-dofbot_ia">3. Algorithme de tri</h3>
            <p>
              Le script <strong><em>garbage_sorting.py</em></strong> centralise l‚Äôensemble des op√©rations n√©cessaires au tri automatique des d√©chets par le Dofbot.
              Il combine la d√©tection via YOLOv5 et les mouvements du bras robotis√©, tout en g√©rant la stabilit√© et la r√©p√©tabilit√© des op√©rations.
            </p>

            <details class="code-container">
              <summary>garbage_sorting.py</summary>
              <div class="btn-bar">
                <button onclick="copierCode('garbage_sorting-py', this)">üìã</button>
                <button onclick="telechargerCode('garbage_sorting-py', 'garbage_sorting.py', 'text/py', this)">ü°á</button>
              </div>
              <pre><code class="hljs" id="garbage_sorting-py">
#!/usr/bin/env python
# coding: utf-8

import threading
import cv2 as cv
from time import sleep
from garbage_identify import garbage_identify
from garbage_grap import garbage_grap_move


class garbage_sorting:
    def __init__(self):
        self.name_tmp = None
        self.num = 0
        self.status = 'waiting'
        self.nbr_detection = 2

        self.garbage_identify = garbage_identify()
        self.garbage_identify.warmup()

        self.arm = garbage_grap_move()
        self.arm.mvt_pos_initial()

    def single_garbage_run(self, image, piece_present=False):
        self.frame = cv.resize(image, (640, 480))

        if not self.garbage_identify.model_ready:
            cv.putText(
                self.frame,
                "MODEL LOADING...",
                (150, 50),
                cv.FONT_HERSHEY_SIMPLEX,
                1,
                (0, 0, 255),
                2
            )
            return self.frame

        if not piece_present:
            self.arm.arm.Arm_RGB_set(0, 0, 0) #RGB off
            cv.putText(
                self.frame,
                "WAITING FOR PIECE",
                (150, 50),
                cv.FONT_HERSHEY_SIMPLEX,
                1,
                (0, 0, 255),
                2
            )
            return self.frame

        if self.status == 'waiting':
            self.garbage_getName()

        return self.frame


    def garbage_getName(self):
        if self.status != "waiting":
            return

        self.frame, msg = self.garbage_identify.garbage_run(self.frame)
        self.arm.arm.Arm_RGB_set(255, 165, 0) #RGB orange

        name = "None"
        for key in msg.keys():
            name = key

        if name == self.name_tmp and name != "None":
            self.num += 1
            if self.num >= self.nbr_detection:
                self.status = 'running'
                threading.Thread(
                    target=self.sorting_garbage,
                    args=(name,),
                    daemon=True
                ).start()
                self.num = 0
        else:
            self.name_tmp = name
            self.num = 0

    def sorting_garbage(self, name):
        self.arm.arm.Arm_Buzzer_On(1)
        sleep(0.5)

        if name == "dangereux":
            self.arm.mvt_dangereux()
        elif name == "menagers":
            self.arm.mvt_menagers()
        elif name == "recyclables":
            self.arm.mvt_recyclables()
        else:
            self.arm.mvt_others()

        self.arm.mvt_pos_initial()
        self.status = 'waiting'
              </code></pre>
            </details>

            <h4>a. Detection d'objets</h4>
              <ul>
                <li>
                  Apr√®s l‚Äôinitialisation du syst√®me, le flux vid√©o est analys√© image par image.
                </li>
                <li>
                  Le mod√®le YOLO retourne un dictionnaire msg contenant les objets d√©tect√©s et leurs coordonn√©es dans l‚Äôimage.
                </li>
                <li>
                  La m√©thode garbage_getName v√©rifie la d√©tection sur plusieurs frames cons√©cutives afin d‚Äô√©viter les faux positifs.
                </li>
              </ul>

            <h4>b. Validation de la d√©tection</h4>
              <ul>
                <li>
                  <strong>name_tmp</strong> : m√©morise la derni√®re classe d√©tect√©e.
                </li>
                <li>
                  <strong>num</strong> : compteur d‚Äôapparitions cons√©cutives.
                </li>
                <li>
                  <strong>nbr_detection</strong> : nombre minimal de d√©tections cons√©cutives pour valider l‚Äôobjet.
                </li>
              </ul>
              <p>
                Si le m√™me objet est d√©tect√© sur au moins nbr_detection frames, la d√©tection est consid√©r√©e valide et le tri peut commencer.
                <br>
                Sinon, le syst√®me continue de scanner le flux vid√©o.
              </p>

              <h4>c. Tri et mouvement</h4>
              <ul>
                <li>
                  Une fois la d√©tection valid√©e, le tri est d√©clench√© dans un thread s√©par√©, pour ne pas bloquer la d√©tection en continu.
                </li>
                <li>
                  L‚Äôobjet est tri√© selon sa cat√©gorie (dangereux, menagers, recyclables, autres).
                </li>
                <li>
                  Le bras retourne ensuite automatiquement √† la position initiale.
                </li>
              </ul>

            <h4>d. Logique de fonctionnement</h4>
              <ul>
                L‚Äôalgorithme suit une boucle continue permettant au Dofbot de d√©tecter et trier les d√©chets automatiquement :
                <li>
                  Capture d‚Äôimage : le convoyeur est scann√© en temps r√©el pour d√©tecter la pr√©sence d‚Äôobjets.
                </li>
                <li>
                  D√©tection des objets : le script identifie les objets pr√©sents et enregistre leur classe.
                </li>
                <li>
                  Validation de la d√©tection : un objet doit √™tre d√©tect√© sur plusieurs frames cons√©cutives pour √™tre consid√©r√© comme valide.
                </li>
                <li>
                  Tri via le bras robotis√©: l'objet est tri√© √† la suite d'une sequence de mouvement du bras.
                </li>
                <li>
                  Signal et Visualisation : le buzzer du bras indique le d√©but de l‚Äôop√©ration, et la led RGB indique le type de d√©chet en cours de traitement.
                </li>
                <li>
                  R√©p√©tition : le syst√®me revient √† l‚Äô√©tat ‚Äúattente‚Äù pour continuer la surveillance du convoyeur.
                </li>
              </ul>

              <p>
                Cette logique garantit un tri fiable, pr√©cis et visuellement identifiable, tout en maintenant la d√©tection active en continu.
              </p>

            <h4>e. Test sur Jupyter</h4>
              <p>
                Les s√©quences de d√©tection et de mouvements ont √©t√© test√©es et valid√©es via Jupyter Notebook.
              </p>

              <details class="code-container">
                <summary>Garbage_sorting.ipynb</summary> 
                <div class="btn-bar">
                  <button onclick="copierCode('Garbage_sorting-ipynb', this)">üìã</button> 
                  <button onclick="telechargerCode('Garbage_sorting-ipynb', 'Garbage_sorting.ipynb', 'application/json', this)">ü°á</button> 
                </div>
                <pre><code class="hljs" id="Garbage_sorting-ipynb">
# ### Import head file

#!/usr/bin/env python
# coding: utf-8
import Arm_Lib
import cv2 as cv
import threading
from time import sleep
import ipywidgets as widgets
from IPython.display import display
from garbage_sorting import garbage_sorting

# ### Create the instance and initialize the parameters
sorting_garbage = garbage_sorting()

model = "General"

# ### Initialize DOFBOT position
sorting_garbage.arm.mvt_pos_initial()

# ### Create controls
button_layout      = widgets.Layout(width='320px', height='60px', align_self='center')
output = widgets.Output()
# exit
exit_button = widgets.Button(description='Exit', button_style='danger', layout=button_layout)
imgbox = widgets.Image(format='jpg', height=480, width=640, layout=widgets.Layout(align_self='center'))
controls_box = widgets.VBox([imgbox, exit_button], layout=widgets.Layout(align_self='center'))

def exit_button_Callback(value):
    global model
    model = 'Exit'
    with output: print(model)
exit_button.on_click(exit_button_Callback)

# ### Main process

def camera():
    # Open camera
    capture = cv.VideoCapture(0)
    # The loop is executed when the camera is opened normally 
    while capture.isOpened():
        try:

            _, img = capture.read()

            img = cv.resize(img, (640, 480))
            img = sorting_garbage.single_garbage_run(img)
            if model == 'Exit':
                cv.destroyAllWindows()
                capture.release()
                break
            imgbox.value = cv.imencode('.jpg', img)[1].tobytes()
        except KeyboardInterrupt:capture.release()

# ### Start

# Place the block in the center of the cross on map.
display(controls_box,output)
threading.Thread(target=camera, ).start()
                </code></pre>
              </details>

              <video controls width=80%>
                <source src="../videos/TRC25/Dofbot_IA/garbage_sorting.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>

          <h3 id="4-dofbot_ia">4. Tests et Demonstration</h3>
            <p>
              Cette phase vise √† √©valuer le fonctionnement global du syst√®me de tri intelligent en conditions r√©elles, en comparant deux modes de communication entre le convoyeur et le Dofbot : 
              communication sans fil et communication filaire via UART. 
              Les tests portent principalement sur <strong>la r√©activit√© du syst√®me</strong>, 
              <strong>la pr√©cision de la prise des d√©chets</strong> et <strong>la stabilit√© des mouvements du bras robotis√©</strong>.
            </p>
            
            <h4>a. Tests avec communication sans fil</h4>  
              <p>
                Dans cette configuration, la synchronisation entre le convoyeur et le Dofbot repose sur <strong>une communication sans fil via TCP</strong>. 
                Le convoyeur agit comme √©metteur d‚Äô√©tat (pr√©sence ou absence d‚Äôun d√©chet), 
                tandis que le Dofbot re√ßoit cette information et d√©clenche la d√©tection puis le tri.
                <br>
                Le script ci-dessous permet de tester le syst√®me complet (d√©tection et tri) avec <strong>une communication TCP</strong> entre le convoyeur et le Dofbot.
              </p>            

              <details class="code-container">
                <summary>main_ssh_tcp.py</summary> 
                <div class="btn-bar">
                  <button onclick="copierCode('main_ssh_tcp-py', this)">üìã</button> 
                  <button onclick="telechargerCode('main_ssh_tcp-py', 'main_ssh_tcp.py', 'text/py', this)">ü°á</button> 
                </div>
                <pre><code class="hljs" id="main_ssh_tcp-py">
#!/usr/bin/env python
# coding: utf-8

import cv2 as cv
import threading
from time import sleep
from garbage_sorting import garbage_sorting
from tcp_server import tcp_Server

sorting_garbage = garbage_sorting()

# ----- TCP -----
tcp = tcp_Server()
tcp.start_server()


def tcp_loop():
    while True:
        tcp.listen()
        sleep(0.01)


threading.Thread(target=tcp_loop, daemon=True).start()

# ----- CAMERA -----


def camera():
    capture = cv.VideoCapture(0)

    if not capture.isOpened():
        print("Erreur cam√©ra")
        return

    while True:
        ret, img = capture.read()
        if not ret:
            break

        img = cv.resize(img, (640, 480))

        img = sorting_garbage.single_garbage_run(
            img,
            piece_present=tcp.presence
        )

        # cv.imshow("Garbage Sorting", img)

        if cv.waitKey(1) & 0xFF == ord('q'):
            break

    capture.release()
    cv.destroyAllWindows()


camera()
                </code></pre>
              </details>

              <p>
                Afin de simplifier le d√©marrage du syst√®me de tri avec communication sans fil (TCP) et d‚Äô√©viter la r√©p√©tition des m√™mes commandes, un <strong>script Bash de lancement</strong> a √©t√© mis en place.
              </p>

              <details class="code-container">
                <summary>launch_sorting_tcp.sh</summary> 
                <div class="btn-bar">
                  <button onclick="copierCode('launch_sorting_tcp-sh', this)">üìã</button> 
                  <button onclick="telechargerCode('launch_sorting_tcp-sh', 'launch_sorting_tcp-sh', 'text/sh', this)">ü°á</button> 
                </div>
                <pre><code class="hljs" id="launch_sorting_tcp-sh">
#!/bin/bash

DEVICE="/dev/video0"

SCRIPT="/home/jetson/b2ms/garbage_sorting_conv/main_ssh_tcp.py"

echo "V√©rification de l'utilisation de $DEVICE..."

# R√©cup√©rer les PID qui utilisent /dev/video0 (la camera)
PIDS=$(fuser $DEVICE 2>/dev/null)

if [ -n "$PIDS" ]; then
    echo "Processus utilisant $DEVICE : $PIDS"
    for PID in $PIDS; do
        echo "Killing PID $PID"
        sudo kill -9 "$PID"
    done
    sleep 1
else
    echo "Aucun processus n'utilise $DEVICE"
fi

echo "Execution de garbage_sorting_comm_v3/main_ssh.py ..."
python "$SCRIPT"
                </code></pre>
              </details>

              <video controls width=80%>
                <source src="../videos/TRC25/Dofbot_IA/garbage_sorting_with_conv1.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>

              <p>
                Lors des premiers essais bas√©s sur une communication sans fil entre le convoyeur et le Dofbot, plusieurs limites ont √©t√© observ√©es :
                <ul>
                  <li>
                    <strong>Une latence notable</strong> entre l‚Äôenvoi du signal par le convoyeur et la r√©action du Dofbot, impactant la synchronisation entre d√©tection et action.
                  </li>
                  <li>
                    Les d√©chets ont tendance √† <strong>ne pas √™tre correctement centr√©s</strong> sur la zone de pr√©hension, ce qui entra√Æne des √©checs de saisie par la pince.
                  </li>
                  <li>
                    Le temps global du cycle de tri est relativement √©lev√©, d√ª √†  
                    la dur√©e du convoyage et la lenteur volontaire des mouvements du bras afin de pr√©server la stabilit√©.
                  </li>
                </ul>
              </p>

              <p>
                Ces contraintes montrent que, bien que fonctionnelle, la communication sans fil n‚Äôest pas optimale pour un tri pr√©cis et r√©p√©table dans cette configuration.
                <br>
                Ce mode reste n√©anmoins int√©ressant pour une architecture distribu√©e ou lorsque le c√¢blage n‚Äôest pas possible.
              </p>

            <h4>b. Tests avec communication UART</h4>
              <p>
                Dans cette configuration, la communication entre le convoyeur et le Dofbot est assur√©e par <strong>une liaison filaire UART</strong>. 
                Le convoyeur (Arduino) envoie directement l‚Äô√©tat de pr√©sence d‚Äôun d√©chet au Dofbot, permettant une synchronisation plus rapide et plus fiable du processus de tri.
                <br>
                Le script suivant permet de tester le tri automatique en utilisant <strong>une communication filaire UART</strong> entre le convoyeur et le Dofbot.
              </p>

              <details class="code-container">
                <summary>main_ssh_uart.py</summary> 
                <div class="btn-bar">
                  <button onclick="copierCode('main_ssh_uart-py', this)">üìã</button> 
                  <button onclick="telechargerCode('main_ssh_uart-py', 'main_ssh_uart.py', 'text/py', this)">ü°á</button> 
                </div>
                <pre><code class="hljs" id="main_ssh_uart-py">
#!/usr/bin/env python
# coding: utf-8

import cv2 as cv
import threading
from time import sleep
from garbage_sorting import garbage_sorting
from uart_Arduino import uart_Arduino

sorting_garbage = garbage_sorting()

# ----- Communication -----
communication = uart_Arduino()
communication.start()


def communication_loop():
    while True:
        communication.listen()
        sleep(0.01)


threading.Thread(target=communication_loop, daemon=True).start()

# ----- CAMERA -----


def camera():
    capture = cv.VideoCapture(0)

    if not capture.isOpened():
        print("Erreur cam√©ra")
        return

    while True:
        ret, img = capture.read()
        if not ret:
            break

        img = cv.resize(img, (640, 480))

        img = sorting_garbage.single_garbage_run(
            img,
            piece_present=communication.presence
        )

        # cv.imshow("Garbage Sorting", img)

        if cv.waitKey(1) & 0xFF == ord('q'):
            break

    capture.release()
    cv.destroyAllWindows()


camera()
                </code></pre>
              </details>

              <p>
                Un script de lancement √©quivalent est utilis√© pour la version UART, en adaptant simplement le chemin vers le fichier Python principal correspondant.  
              </p>

              <details class="code-container">
                <summary>launch_sorting_uart.sh</summary> 
                <div class="btn-bar">
                  <button onclick="copierCode('launch_sorting_uart-sh', this)">üìã</button> 
                  <button onclick="telechargerCode('launch_sorting_uart-sh', 'launch_sorting_uart-sh', 'text/sh', this)">ü°á</button> 
                </div>
                <pre><code class="hljs" id="launch_sorting_uart-sh">
#!/bin/bash

DEVICE="/dev/video0"

SCRIPT="/home/jetson/b2ms/garbage_sorting_conv/main_ssh_uart.py"

echo "V√©rification de l'utilisation de $DEVICE..."

# R√©cup√©rer les PID qui utilisent /dev/video0 (la camera)
PIDS=$(fuser $DEVICE 2>/dev/null)

if [ -n "$PIDS" ]; then
    echo "Processus utilisant $DEVICE : $PIDS"
    for PID in $PIDS; do
        echo "Killing PID $PID"
        sudo kill -9 "$PID"
    done
    sleep 1
else
    echo "Aucun processus n'utilise $DEVICE"
fi

echo "Execution de garbage_sorting_comm_v3/main_ssh.py ..."
python "$SCRIPT"
                </code></pre>
              </details>

              <video controls width=80%>
                <source src="../videos/TRC25/Dofbot_IA/garbage_sorting_with_conv2.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>

              <p>
                Les essais r√©alis√©s avec une communication filaire via UART ont permis d‚Äôam√©liorer les performances globales du syst√®me :
                <ul>
                  <li>
                    La communication est <strong>plus r√©active</strong>, avec une r√©duction perceptible du temps de r√©ponse entre le convoyeur et le Dofbot.
                  </li>
                  <li>
                    Il a √©t√© possible de r√©duire <strong>le nombre de d√©tections cons√©cutives</strong> n√©cessaires √† la validation d‚Äôun objet, ainsi que <strong>le temps des mouvements du bras</strong>, sans compromettre la stabilit√©.
                  </li>
                  <li>
                    Une pi√®ce m√©canique ajout√©e en fin de convoyeur permet d√©sormais <strong>de centrer correctement les d√©chets</strong> avant la phase de prise.
                  </li>
                  <li>
                    Gr√¢ce √† ce centrage, le bras robotis√© parvient <strong>√† saisir les d√©chets de mani√®re fiable et r√©p√©table</strong>.
                  </li>
                </ul>
              </p>

              <p>
                Ces am√©liorations rendent le syst√®me <strong>plus robuste et mieux adapt√©</strong> √† une utilisation continue.
                <br>
                <strong>N√©anmoins</strong>, d'autres optimisations restent √† envisager afin d‚Äôacc√©l√©rer davantage le cycle de tri.
              </p>

            <h3 id="5-dofbot_ia">5. Suivi et gestion des historiques</h3>
              <p>
                √Ä la suite des diff√©rentes phases de tests et de d√©monstration, il est apparu n√©cessaire d‚Äôajouter des fonctionnalit√©s permettant de suivre et enregistrer l‚Äôhistorique des d√©chets tri√©s.
                Ces ajouts visent √† am√©liorer la tra√ßabilit√©, la visualisation des performances du syst√®me, ainsi que l‚Äôexploitation des donn√©es sur une p√©riode donn√©e.
              </p>

              <p>
                L‚Äôint√©gration de l‚Äôhistorique permet de :
                <ul>
                  <li>
                    Conserver une trace des d√©chets d√©tect√©s et tri√©s.
                  </li>
                  <li>
                    Analyser la r√©partition des cat√©gories de d√©chets.
                  </li>
                  <li>
                    Faciliter le d√©bogage et l‚Äôam√©lioration continue du tri.
                  </li>
                  <li>
                    Disposer de donn√©es exploitables pour des statistiques ou une interface future.
                  </li>
                </ul>
              </p>

              <h4>a. Principe de fonctionnement</h4>
                <p>
                  √Ä chaque op√©ration de tri valid√©e :
                  <ul>
                    <li>
                      Le type de d√©chet identifi√© est enregistr√©.
                    </li>
                    <li>
                      La date et l‚Äôheure de la d√©tection sont associ√©es √† l‚Äô√©v√©nement.
                    </li>
                    <li>
                      L‚Äôinformation est stock√©e localement dans une structure d√©di√©e.
                    </li>
                    <li>
                      Les donn√©es peuvent √™tre consult√©es, analys√©es et utilis√©es ult√©rieurement.
                    </li>
                  </ul>
                </p>

                <p>
                  Ce m√©canisme est ind√©pendant du mode de communication utilis√© entre le Dofbot et le convoyeur.
                </p>

              <h4>b. Structure</h4>
                <p>
                  Un module d√©di√© √† la gestion de l‚Äôhistorique est int√©gr√© au syst√®me.
                  <br>
                  Il assure:
                  <ul>
                    <li>
                      l‚Äôenregistrement des √©v√©nements de tri,
                    </li>
                    <li>
                      la mise √† jour des compteurs par cat√©gorie,
                    </li>
                    <li>
                      la persistance des donn√©es (fichier local).
                    </li>
                  </ul>
                </p>

                <details class="code-container">
                  <summary>garbage_history.py</summary> 
                  <div class="btn-bar">
                    <button onclick="copierCode('garbage_history-py', this)">üìã</button> 
                    <button onclick="telechargerCode('garbage_history-py', 'garbage_history-py', 'text/py', this)">ü°á</button> 
                  </div>
                  <pre><code class="hljs" id="garbage_history-py">
import json
import os
from datetime import datetime


class GarbageHistory:
    GARBAGE_TYPES = ["dangereux", "menagers", "recyclables"]

    def __init__(self, file_path="garbage_history.json"):
        self.file_path = file_path
        self.data = {
            "total": 0,
            "by_type": {g: 0 for g in self.GARBAGE_TYPES},
            "history": []
        }
        self.load()

    def load(self):
        if os.path.exists(self.file_path):
            with open(self.file_path, "r") as f:
                self.data = json.load(f)

            for g in self.GARBAGE_TYPES:
                if g not in self.data["by_type"]:
                    self.data["by_type"][g] = 0
        else:
            self.save()

    def save(self):
        with open(self.file_path, "w") as f:
            json.dump(self.data, f, indent=4)

    def add(self, garbage_type):
        self.data["total"] += 1
        self.data["by_type"][garbage_type] += 1

        self.data["history"].append({
            "type": garbage_type,
            "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })

        self.save()
        self.display_last(garbage_type)

    def display_last(self, garbage_type):
        print(f"\nNouveau d√©chet tri√© : {garbage_type}")
        print("R√©partition :")
        for g in self.GARBAGE_TYPES:
            print(f"   - {g} : {self.data['by_type'][g]} ({(self.data['by_type'][g]/self.data['total'])*100.0})%")
        print(f"Total d√©chets : {self.data['total']}")
        print("-" * 40)

    def display_all(self):
        print("\nHISTORIQUE COMPLET")
        for i, item in enumerate(self.data["history"], 1):
            print(f"{i}. {item['time']}: {item['type']}")

    def reset(self):
        confirm = input("Confirmer le reset ? (oui/non) : ")
        if confirm.lower() == "oui":
            self.data = {
                "total": 0,
                "by_type": {g: 0 for g in self.GARBAGE_TYPES},
                "history": []
            }
            self.save()
            print("Historique r√©initialis√©")
        else:
            print("Reset annul√©")
                  </code></pre>
                </details>

                <h4>c. Int√©gration dans l'algorithme de tri</h4>
                  <p>
                    Le module <strong><em>garbage_history</em></strong> est appel√© apr√®s une op√©ration de tri r√©ussie, afin d‚Äôenregistrer uniquement les d√©chets effectivement manipul√©s par le bras.
                  </p>

                  <div class="code-container">
                    <p>Extrait d'int√©gration dans <strong><em>garbage_sorting.py</em></strong></p> 
                    <div class="btn-bar">
                      <button onclick="copierCode('extrait_garbage_sorting-py', this)">üìã</button> 
                    </div>
                    <pre><code class="hljs" id="extrait_garbage_sorting-py">
from garbage_history import GarbageHistory

class garbage_sorting:
    def __init__(self):
        ...
        self.history = GarbageHistory()

    ...

    def sorting_garbage(self, name):
    self.arm.arm.Arm_Buzzer_On(1)
    sleep(0.5)

    if name == "dangereux":
        self.arm.mvt_dangereux()
    elif name == "menagers":
        self.arm.mvt_menagers()
    elif name == "recyclables":
        self.arm.mvt_recyclables()
    else:
        self.arm.mvt_others()

    # Enregistrement dans l‚Äôhistorique
    self.history.add(name)

    self.arm.mvt_pos_initial()
    self.status = 'waiting'
                    </code></pre>
                  </div>

                  <div class="image-container">
                    <img src="../images/TRC25/Dofbot_IA/terminal.webp" alt="terminal" class="image-small"/>
                  </div>

                <h4>d. Interaction utilisateur</h4>
                  <p>
                    En compl√©ment de l‚Äôenregistrement automatique des d√©chets tri√©s, une interface en ligne de commande a √©t√© int√©gr√©e dans le script <strong><em>garbage_sorting.py</em></strong>.
                    <br>
                    Elle permet √† l‚Äôutilisateur d‚Äôinteragir directement avec l‚Äôhistorique pendant l‚Äôex√©cution du programme.
                  </p>

                  <p>
                    Une boucle d√©di√©e attend l‚Äôinitialisation du syst√®me de vision, puis permet √† l‚Äôutilisateur d‚Äôinteragir en temps r√©el via la console afin de consulter, 
                    r√©initialiser ou afficher l‚Äôhistorique des d√©chets tri√©s, sans interrompre le processus de d√©tection et de tri automatique.
                  </p>

                  <div class="code-container">
                    <p>user_commands dans <strong><em>garbage_sorting.py</em></strong></p> 
                    <div class="btn-bar">
                      <button onclick="copierCode('user_commands-py', this)">üìã</button> 
                    </div>
                    <pre><code class="hljs" id="user_commands-py">
    ...
    def user_commands(self):
        while not self.opencv_ready:
            sleep(0.2)
            
        while True:
            cmd = input("\nCommande (r=reset, s=stats, h=historique) : ")
            if cmd == "r":
                self.history.reset()
            elif cmd == "s":
                print(self.history.data)
            elif cmd == "h":
                self.history.display_all()
                    </code></pre>
                  </div>

                  <table>
                    <thead>
                      <tr>
                        <th>Commande</th>
                        <th>Action</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><strong>r</strong></td>
                        <td>R√©initialise l‚Äôhistorique des d√©chets</td>
                      </tr>
                      <tr>
                        <td><strong>s</strong></td>
                        <td>Affiche les statistiques (compteurs par cat√©gorie)</td>
                      </tr>
                      <tr>
                        <td><strong>h</strong></td>
                        <td>Affiche l‚Äôhistorique complet des d√©chets tri√©s</td>
                      </tr>
                    </tbody>
                  </table> 

                  <div class="image-container">
                    <img src="../images/TRC25/Dofbot_IA/terminal_commande.webp" alt="terminal" class="image-mean"/>
                  </div>

              <p class="txt_center">
                <a href="https://github.com/TekBot-Robotics-Challenge/2025-Team-B2MS_CleanTech-Docs/raw/refs/heads/main/TRC25/b2ms_Dofbot_IA.zip" target="_blank" class="download-link">
                  T√©l√©charger b2ms_Dofbot_IA.zip
                </a>
              </p>
              
        </div>
      </section>
    </main>
  </div>

  <footer>
    Copyright ¬© 2025 Team B2MS CleanTech. All rights reserved.
  </footer>

  <script src="../scripts/script_commun.js"></script>
  <script src="../scripts/script_code.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</body>
</html>
