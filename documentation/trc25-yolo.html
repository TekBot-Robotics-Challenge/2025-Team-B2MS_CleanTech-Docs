<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css"
    />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../styles/style-commun.css" />
  <link rel="stylesheet" href="../styles/style-code.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/srcery.min.css">
  <link rel="icon" type="image/x-icon" href="../logo/favicon.ico">
  <title>R√©silience Urbaine - B2MS CleanTech</title>
</head>
<body>
  <header class="header">
    <div class="section__container header__container_domaine">
      <h2>R√©silience Urbaine</h2>
    </div>
  </header>

  <div class="layout-container">
    <!-- Sidebar -->
    <div class="sidebar">
      <div class="sidebar-toggle" id="sidebarToggle">
        <i class="fas fa-bars"></i>
      </div>

      <nav class="main-nav">
        <a href=".././" class="nav-link"><i class="fas fa-home"></i> Accueil</a>
        <a href="./it" class="nav-link"><i class="fas fa-desktop"></i> IT</a>
        <a href="./electronique" class="nav-link"><i class="fas fa-microchip"></i> √âlectronique</a>
        <a href="./mecanique" class="nav-link"><i class="fas fa-cogs"></i> M√©canique</a>
        <a href="./convoyeur" class="nav-link"></i><i class="fas fa-forward"></i> Convoyeur</a>

        <div class="nav-section">
          <a href="./trc25" style="text-decoration: none;"><button class="tab-button active"><i class="fas fa-robot"></i> TRC25</button> </a>       
          <div class="sub-links">
            <a class="subtab-button" href="./trc25-convoyeur">Convoyeur 2.0</a>
            <a class="subtab-button" href="./trc25-dofbot">Prise en main du Dofbot</a>
            <a class="subtab-button" href="./trc25-communication">Communication Dofbot - Convoyeur</a>
            <a class="subtab-button active" href="./trc25-yolo">Mod√®le de d√©tection YOLO</a>
            <nav class="project-subnav active" id="subnav-yolo">
              <a href="#1-yolo">1. Configuration de l'environnement</a>
              <a href="#2-yolo">2. Collecte et annotation des images</a>
              <a href="#3-yolo">3. Structure des dossiers</a>
              <a href="#4-yolo">4. Codage sur Jupyter</a>
              <a href="#6-yolo">5. Scripts python</a>
              <a href="#7-yolo">6. YOLOv5 vs YOLOv8</a>
            </nav>
            <a class="subtab-button" href="./trc25-dofbot_ia">D√©tection et tri - Dofbot</a>
            <a class="subtab-button" href="./trc25-ros">ROS</a>
            <a class="subtab-button" href="./trc25-meca">Syst√®me de ramassage</a>
          </div>
        </div>

      </nav>
    </div>

    <!-- Contenu principal -->
    <main class="main-content">
      <section class="tab-content active">
        <div class="subtab-content active">
          <h2 id="yolo" class="projet-titre">D√©tection des d√©chets</h2>
            <p>
              Le mod√®le de d√©tection, d'indentification et de classement des d√©chets se fera via YOLO.
              <br>
              <a href="https://docs.ultralytics.com/fr/#yolo-a-brief-history" target="_blank">YOLO(You Only Look Once)</a>
               est un r√©seau de neurones convolutionnel (CNN) con√ßu pour la d√©tection d‚Äôobjets en temps r√©el.
              <br>
              Parmi les diff√©rentes versions de YOLO, YOLOv8 a √©t√© retenue comme r√©f√©rence dans cette documentation, car elle pr√©sente un meilleur compromis entre vitesse d‚Äôex√©cution et exactitude des d√©tections.  
              Par la suite, une comparaison entre YOLOv5 et YOLOv8 sera faite afin de d√©terminer la version la plus appropri√©e pour le <strong>Dofbot Yahboom</strong>.
            </p>
          <h3 id="1-yolo">1. Configuration de l'environnement</h3>
            <p>
              Les d√©chets seront d√©tect√©s et identifi√©s par un mod√®le entrain√© sur YOLOv8.
              Pour cela, il faut commencer par configurer l'environnement de travail avec <a href="https://www.anaconda.com/download/success">Anaconda</a>.
            </p>
            <h4>a. Installer Anaconda</h4>
              <p>
                Anaconda est une distribution logicielle open-source qui facilite l'installation et la gestion d'environnements Python.
                Il permet d'installer des librairies sans se soucier des conflits avec les librairies d√©j√† existantes.
                <br>
                <ul>
                  <li><a href="https://www.anaconda.com/download/success" target="_blank">T√©l√©charger anaconda</a></li>
                  <li>Installer et suiver les √©tapes d'installation</li>
                  <li>Ouvrir "<strong><em>Anaconda Prompt</em></strong>"</li>
                </ul>
              </p>
            <div class="image-container">
              <img src="../images/TRC25/YOLO/anaconda_prompt.webp" alt="anaconda prompt" class="image-small"/>
            </div>

            <h4>b. Environnement et librairies</h4>
              <p>
                Ayant r√©ussi √† installer anaconda, il est temps de cr√©er l'environnement de travail et d'y installer les librairies n√©cessaires.
                <br>
                <ul>
                  <li>Cr√©ation d'un nouvel environnement</li>
                    <div class="code-container">
                      <div class="btn-bar">
                        <button onclick="copierCode('creation_env', this)">üìã</button>
                      </div>
                      <pre><code class="hljs" id="creation_env">
conda create --name b2ms-env python=3.12 -y
                      </code></pre>
                    </div>
                    <li>Activation de l'environnement cr√©√©</li>
                    <div class="code-container">
                      <div class="btn-bar">
                        <button onclick="copierCode('activation_env', this)">üìã</button>
                      </div>
                      <pre><code class="hljs" id="activation_env">
conda activate b2ms-env
                      </code></pre>
                    </div>
                    <li>Installation de <strong>Ultralytics</strong>: package de librairies python pour le mod√®le YOLO</li>
                    <div class="code-container">
                      <div class="btn-bar">
                        <button onclick="copierCode('ultralytics', this)">üìã</button>
                      </div>
                      <pre><code class="hljs" id="ultralytics">
pip install ultralytics
                      </code></pre>
                    </div>
                    <p>
                      Cette commande va √©galement installer plusieurs librairies: OpenCV, Numpy, Sympy, PyTorch(version sur CPU par d√©faut), etc.
                      <br>
                    </p>
                  <li>Installation de PyTorch avec CUDA v12.4: pour les ordinateurs ayant un GPU</li>
                    <div class="code-container">
                      <div class="btn-bar">
                        <button onclick="copierCode('pytorch_gpu', this)">üìã</button>
                      </div>
                      <pre><code class="hljs" id="pytorch_gpu">
pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
                      </code></pre>
                    </div>
                    <p>
                      Cela permet d'entra√Æner le mod√®le sur un ordinateur √©quip√© d'un GPU.
                      Pour les ordinateurs qui n'en poss√®dent pas, <a href="https://colab.google/" target="_blank">Google Colab</a> 
                      offre la possibilit√© d'effectuer l'entra√Ænement √† distance gr√¢ce √† ses ressources GPU.
                    </p>
                    <li>V√©rification de PyTorch avec CUDA v12.4: cette commande affichera le GPU du syst√®me si CUDA a √©t√© install√© correctement.</li>
                    <div class="code-container">
                      <div class="btn-bar">
                        <button onclick="copierCode('pytorch_check', this)">üìã</button>
                      </div>
                      <pre><code class="hljs" id="pytorch_check">
python -c "import torch; print(torch.cuda.get_device_name(0))"
                      </code></pre>
                    </div>
                </ul>
              </p>

          <h4>c. Anaconda Navigator et Jupyter</h4>
            <p>
              Pour pouvoir utiliser certains logiciels sur Anaconda Navigator, il faut les ajouter sur l'environnement de travail.
              Il faut commencer par <strong>activer</strong> l'environnement d√©sir√©.         
              <div class="code-container">
                <div class="btn-bar">
                  <button onclick="copierCode('ipykernel', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="ipykernel">
conda activate b2ms-env
conda install ipykernel #noyau Python utilis√© par Jupyter
pip install ipykernel
python -m ipykernel install --user --name=b2ms-env --display-name "B2MS-Env"
conda install jupyter
                </code></pre>
              </div>
              <p class="txt_center">
                <strong>Jupyter notebook</strong> sera notamment tr√®s utile lors de la phase de codage.
              </p>
            </p>

          <h3 id="2-yolo">2. Collecte et annotation des images</h3>
            <p>
              Pour pouvoir entrainer le mod√®le de d√©tection, il faut un dataset: <strong>des images √©tiquett√©es</strong>.
              Un jeu de 100 √† 200 images constitue g√©n√©ralement un bon point de d√©part pour d√©velopper un prototype de mod√®le, mais il faut l'enrichir le plus possible.
              Les images utilis√©es pour l'entra√Ænement doivent pr√©senter les objets dans <strong>diff√©rents contextes</strong>, avec <strong>des angles de vue</strong> et des <strong>√©clairages vari√©s</strong>, similaires √† ceux rencontr√©s dans les conditions r√©elles d‚Äôutilisation.
              Il est √©galement recommand√© d'ajouter des objets non pertinents dans les images afin d'aider le mod√®le √† diff√©rencier les √©l√©ments √† d√©tecter de ceux √† ignorer.
            </p>
            <p class="txt_center">
              <a href="https://github.com/TekBot-Robotics-Challenge/2025-Team-B2MS_CleanTech-Docs/raw/refs/heads/main/TRC25/b2ms_YOLO/ressources_images.zip" target="_blank" class="download-link">T√©l√©charger les images √† annoter</a>
            </p>

            <p>
              Une fois les images prises, il faut les √©tiquetter gr√¢ce √† des outils comme <a href="https://labelstud.io/">Label Studio</a> ou <a href="https://universe.roboflow.com/">Roboflow</a>.
            </p>

            <h4>a. Label Studio</h4>
              <p>
                Label Studio, open-source et facile √† prendre en main, a √©t√© choisi pour √©tiquetter les images.
              </p>
              <ul>
                <li>Installation de Label Studio:</li>
                  <div class="code-container">
                    <div class="btn-bar">
                      <button onclick="copierCode('label_studio', this)">üìã</button>
                    </div>
                    <pre><code class="hljs" id="label_studio">
conda activate b2ms-env
pip install label-studio
                    </code></pre>
                  </div>
                <li>Lancement de Label Studio:</li>
                  <div class="code-container">
                  <div class="btn-bar">
                    <button onclick="copierCode('label_studio_start', this)">üìã</button>
                  </div>
                  <pre><code class="hljs" id="label_studio_start">
label-studio start
                  </code></pre>
                </div>
              </ul>
              <p>
                Une fois l'interface ouverte, il suffit de saisir une adresse e-mail et un mot de passe (r√©els ou fictifs) pour y acc√©der.
                <ul>
                  Exemple:
                  <li>
                    <strong>Identifiant(mail):</strong> b2ms@gmail.com
                  </li>
                  <li>
                    <strong>Mot de passe:</strong> #TRC2025
                  </li>
                </ul>
              </p>
              <p>
                Toutes les donn√©es sont stock√©es localement.
              </p>
            <figure>
              <video controls width=auto>
                <source src="../videos/TRC25/YOLO/label_studio.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>
              <p class="txt_center">Etiquetage de certaines images avec Label Studio</p>
            </figure>

            <p>
              Apr√®s avoir √©tiquett√© les images, il faut les exporter. Voici √† quoi devrait ressembler le contenu du fichier <strong><em>zip</em></strong> export√©:
            </p>
            <div class="arbo">
              <ul>
                  <li class="folder">images
                    <ul>
                      <li class="img">image1.jpg</li>
                      <li class="img">image2.jpg</li>
                      <li class="img">image3.jpg</li>
                      <li class="img">...</li>
                    </ul>
                  <li class="folder">labels
                    <ul>
                      <li class="txt">image1.txt</li>
                      <li class="txt">image2.txt</li>
                      <li class="txt">image3.txt</li>
                      <li class="txt">...</li>
                    </ul>
                  <li class="txt">classes.txt</li>      
                </ul>
              </ul>
            </div>

          <h4>b. Roboflow</h4>
              <p>
                Roboflow Annotate est un outil en ligne pour l'√©tiquetage d'images pour diverses t√¢ches de vision par ordinateur.
                Il offre plusieurs fonctionnalit√©s notamment l'auto-√©tiquetage et l'ajout des diff√©rents param√©tres de perspctive et de luminosit√©.
                <br>
                Les fichiers √©tiquet√©s pr√©c√©demment sont import√©s dans Roboflow afin d'utiliser les fonctionnalit√©s suivantes:
                <table>
                  <thead>
                    <tr>
                      <th>Cat√©gories</th>
                      <th>Transformations</th>
                      <th>Descriptions</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>G√©om√©triques</td>
                      <td> 
                        <ul class="dash-list">
                          <li>Retournement(flip)</li>
                          <li>Rotation</li>
                          <li>Recadrage(crop)</li>
                          <li>Cisaillement(shear)</li>
                        </ul> 
                      </td>
                      <td>Apprendre au mod√®le √† reconna√Ætre les objets sous diff√©rentes orientations et cadrages.</td>
                    </tr>
                    <tr>
                      <td>Photom√©triques</td>
                      <td>
                        <ul class="dash-list">
                          <li>Teinte(hue)</li>
                          <li>Luminosit√©(brightness)</li>
                          <li>Exposition(exposure)</li>
                          <li>Flou(blur)</li>
                          <li>Bruit(noise)</li>
                        </ul> 
                      </td>
                      <td>Rendre le mod√®le robuste aux changements de lumi√®re, couleur et qualit√© d‚Äôimage.</td>
                    </tr>
                  </tbody>
                </table> 
              </p>

              <figure>
              <video controls width=auto>
                <source src="../videos/TRC25/YOLO/roboflow.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>
              <p class="txt_center">Utilisation de Roboflow</p>
            </figure>
          
            <p>
              Par la suite, nous avons continu√© √† utiliser Roboflow, car il permet non seulement de collaborer en √©quipe, 
              mais aussi d'annoter automatiquement les images import√©es.
            </p>

            <p class="txt_center">
              <a href="https://github.com/TekBot-Robotics-Challenge/2025-Team-B2MS_CleanTech-Docs/raw/refs/heads/main/TRC25/b2ms_YOLO/2000_labeled_images_for_YOLO.zip" target="_blank" class="download-link">T√©l√©charger les images annot√©es pour YOLOv8</a>
            </p>

          <h3 id="3-yolo">3. Structure des dossiers</h3>
            <p>
              Ultralytics exige une structure de dossiers particuli√®re pour les donn√©es d'entra√Ænement des mod√®les. 
              Le dossier racine est nomm√© <strong>data</strong>. 
              √Ä l'int√©rieur, il y a deux dossiers principaux :
              <ul>
                <li>
                  <strong>train</strong> contient les images et les labels utilis√©s pour entra√Æner le mod√®le. 
                  L'algorithme d'entra√Ænement ajuste le mod√®le en fonction des donn√©es pr√©sentes dans ces images.
                </li>
                <li>
                  <strong>valid</strong> contient les images et les labels qui sont utilis√©s p√©riodiquement 
                  pour tester le mod√®le pendant l'entra√Ænement. 
                  √Ä la fin de chaque √©poque d'entra√Ænement, le mod√®le est √©valu√© sur ces images afin de calculer 
                  des m√©triques comme la <strong>pr√©cision</strong>, le <strong>rappel</strong> et le <strong>mAP</strong>.
                </li>
              </ul>
              </p>
              <p>
                En g√©n√©ral, <strong>80%</strong> des images d'un dataset sont plac√©es dans le dossier <strong>train</strong> et <strong>20%</strong> dans <strong>validation</strong>. 
                Dans chacun de ces dossiers se trouvent un dossier images et un dossier labels, qui contiennent respectivement les fichiers d'images et les fichiers d'annotations.
                <br>
                La structure globale des dossiers requise pour entra√Æner les mod√®les YOLO d'Ultralytics est illustr√©e ci-dessous. 
              </p>
              <div class="arbo">
              <ul>
                  <li class="folder">data
                    <ul>
                      <li class="folder">train
                        <ul>
                          <li class="folder">images
                            <ul>
                              <li class="img">image1.jpg</li>
                              <li class="img">image2.jpg</li>
                              <li class="img">image3.jpg</li>
                              <li class="img">...</li>
                            </ul>
                          <li class="folder">labels
                            <ul>
                              <li class="txt">image1.txt</li>
                              <li class="txt">image2.txt</li>
                              <li class="txt">image3.txt</li>
                              <li class="txt">...</li>
                            </ul>
                            <li class="txt">classes.txt</li>      
                        </ul>
                      </li>
                      <li class="folder">valid
                        <ul>
                          <li class="folder">train
                            <ul>
                              <li class="img">image101.jpg</li>
                              <li class="img">image102.jpg</li>
                              <li class="img">image103.jpg</li>
                              <li class="img">...</li>
                            </ul>
                          <li class="folder">labels
                            <ul>
                              <li class="txt">image101.txt</li>
                              <li class="txt">image102.txt</li>
                              <li class="txt">image103.txt</li>
                              <li class="txt">...</li>
                            </ul>
                            <li class="txt">classes.txt</li>  
                          </ul>
                        </li>   
                            <li class="txt" style="display: block">data.yaml</li>      
                        </ul>
                      </li>                    
                    </ul>
                  </li>
                </ul>
              </div>
              <p>
                <strong>NB:</strong> Cette structure peut √™tre effectu√©e manuellement 
                ou automatiquement gr√¢ce √† un script Python pr√©sent√© dans la section suivante.
              </p>

              <p>
                Dans le fichier <strong><em>data.yaml</em></strong>:
              </p>
              <div class="code-container">
                <div class="btn-bar">
                  <button onclick="copierCode('yaml', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="yaml">
path: D:\TRC25\IA\detection\data
train: train\images
val: valid\images

nc: 3

names: [‚Äúdangereux‚Äù, ‚Äùmenagers‚Äù, ‚Äùrecyclables‚Äù]
                </code></pre>
              </div>
              <ul>
                <li><strong>path</strong> indique le chemin absolu du dossier <strong>data</strong></li>
                <li><strong>train</strong> dossier d'entrainement</li>
                <li><strong>val</strong> dossier de validation</li>
                <li><strong>nc</strong> le nombre de classes d'objets √† d√©tecter</li>
                <li><strong>names</strong> les diff√©rentes classes d'objets √† d√©tecter</li>
              </ul>
            
          <h3 id="4-yolo">4. Codage sur Jupyter</h3>
              <p>
                Une fois dans l'environnement virtuel et dans le dossier souhait√© (par exemple <strong>YOLO_detection</strong>), 
                il suffit de lancer <strong>Jupyter Notebook</strong> avec :
              </p>
              <div class="code-container">
                <div class="btn-bar">
                  <button onclick="copierCode('jupyter_notebook', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="jupyter_notebook">
jupyter notebook
                </code></pre>
              </div>
              <p>
                Un onglet s'ouvrira automatiquement dans un navigateur. 
                Il est alors possible de cr√©er un fichier <strong><em>.ipynb</em></strong> pour commencer √† √©crire le code.  
              </p>

            <h4>a. Importation des biblioth√®ques</h4>
              <p>
                La premi√®re √©tape consiste √† importer toutes les biblioth√®ques Python n√©cessaires :
              </p>
              <details class="code-container">
                <summary>
                  import
                </summary>
                <div class="btn-bar">
                  <button onclick="copierCode('import', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="import">
import torch                       # Gestion des tenseurs et ex√©cution des mod√®les YOLO
import cv2                         # Traitement d‚Äôimages (OpenCV)
import matplotlib.pyplot as plt    # Affichage d‚Äôimages, graphiques
import numpy as np                 # Calculs num√©riques
from PIL import Image              # Chargement et manipulation d‚Äôimages (format PIL)
import os                          # Gestion des fichiers/dossiers
from ultralytics import YOLO       # utilisation des mod√®les YOLO
import yaml                        # Lecture/√©criture des fichiers YAML
from pathlib import Path           # Gestion pratique et s√©curis√©e des chemins de fichiers
import random                      # S√©lection al√©atoire
import sys                         # Acc√®s au syst√®me
import shutil                      # manipulation des fichiers/dossiers
from tqdm import tqdm              # Barre de progression
import glob                        # Recherche de fichiers
import pandas as pd                # Gestion et manipulation de donn√©es tabulaires
import math                        # fonctions math√©matiques
                </code></pre>
              </details>
            
            <h4>b. Structure des dossiers</h4>
              <p>
                Cette √©tape consiste √† organiser les dossiers conform√©ment √† la structure d√©crite pr√©c√©demment.
              </p>
              <details class="code-container">
                <summary>
                  organisation des dossiers
                </summary>
                <div class="btn-bar">
                  <button onclick="copierCode('data_structure', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="data_structure">
# Param√®tres
data_path = "./dataset"     # Chemin du dataset d'origine
train_percent = 0.8         # Pourcentage d'images pour l'entra√Ænement (train), le reste sera utilis√© pour la validation (val)


# V√©rification des param√®tres
if not os.path.isdir(data_path):
    print("Le dossier sp√©cifi√© par data_path est introuvable. V√©rifie le chemin et r√©essaie.")
    sys.exit(0)

if train_percent < 0.01 or train_percent > 0.99:
    print("Valeur invalide pour train_percent. Elle doit √™tre comprise entre 0.01 et 0.99.")
    sys.exit(0)

val_percent = 1 - train_percent


# D√©finition des chemins
input_image_path = os.path.join(data_path, "images")  # Dossier contenant les images originales
input_label_path = os.path.join(data_path, "labels")  # Dossier contenant les labels

cwd = os.getcwd()

# Dossiers de sortie
train_img_path = os.path.join(cwd, "data/train/images")
train_label_path = os.path.join(cwd, "data/train/labels")
val_img_path = os.path.join(cwd, "data/valid/images")
val_label_path = os.path.join(cwd, "data/valid/labels")


# Nettoyage et cr√©ation des dossiers
for path in [train_img_path, train_label_path, val_img_path, val_label_path]:
    if os.path.exists(path):
        shutil.rmtree(path)  # Supprime les anciens fichiers pour √©viter les doublons
    os.makedirs(path)  # Cr√©e les nouveaux dossiers vides
    print(f"Dossier pr√™t : {path}")


# R√©cup√©ration des fichiers
img_file_list = [path for path in Path(input_image_path).rglob("*") if path.is_file()]
txt_file_list = [path for path in Path(input_label_path).rglob("*") if path.is_file()]

print(f"\nNombre total d'images : {len(img_file_list)}")
print(f"Nombre total de fichiers d'annotation : {len(txt_file_list)}")


# D√©termination du nombre de fichiers √† d√©placer
file_num = len(img_file_list)
train_num = int(file_num * train_percent)
val_num = file_num - train_num

print(f"\nImages vers train : {train_num}")
print(f"Images vers valid : {val_num}")


# M√©lange al√©atoire des images
random.shuffle(img_file_list)
train_files = img_file_list[:train_num]
val_files = img_file_list[train_num:]

# Train
for img_path in tqdm(train_files, desc="Copie des fichiers train"):
    img_fn = img_path.name
    base_fn = img_path.stem
    txt_fn = base_fn + ".txt"
    txt_path = os.path.join(input_label_path, txt_fn)

    # Copie de l'image
    shutil.copy(img_path, os.path.join(train_img_path, img_fn))

    # Copie du label si disponible
    if os.path.exists(txt_path):
        shutil.copy(txt_path, os.path.join(train_label_path, txt_fn))

# Validation
for img_path in tqdm(val_files, desc="Copie des fichiers validation"):
    img_fn = img_path.name
    base_fn = img_path.stem
    txt_fn = base_fn + ".txt"
    txt_path = os.path.join(input_label_path, txt_fn)

    shutil.copy(img_path, os.path.join(val_img_path, img_fn))

    if os.path.exists(txt_path):
        shutil.copy(txt_path, os.path.join(val_label_path, txt_fn))


print("\nDivision du dataset termin√©e avec succ√®s !")
print(f"Train : {len(os.listdir(train_img_path))} images")
print(f"Validation : {len(os.listdir(val_img_path))} images")
                </code></pre>
              </details>
              <div class="image-container">
                <img src="../images/TRC25/YOLO/data_structure.webp" alt="structure des dossiers" class="image-mean"/>
              </div>

            <h4>c. Configuration de l'entra√Ænement</h4>
              <p>
                Cette √©tape permet de d√©finir le mod√®le YOLO pr√©-entra√Æn√© √† utiliser ainsi que tous les param√®tres 
                n√©cessaires pour l'entra√Ænement(nombre d'√©poques, taille des images, batch, learning rate, etc.).
              </p>
              <details class="code-container">
                <summary>
                  Configuration d'entrainement
                </summary>
                <div class="btn-bar">
                  <button onclick="copierCode('config_train', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="config_train">
# Chargement du mod√®le YOLOv8 pr√©-entra√Æn√©
model = YOLO('yolov8n.pt')  # nano version (plus rapide)

# Configuration de l'entra√Ænement
epochs = 20
imgsz = 640
batch = 16

train_config = {
    'data': 'data.yaml', 
    'epochs': epochs,            
    'imgsz': imgsz,          
    'batch': batch,            
    'device': 0 if torch.cuda.is_available() else 'cpu',
    'workers': 4,           
    'project': 'runs/detect',  # Nom du projet
    'name': 'train',        # Nom de l'exp√©rience
    'save_period': 5,       # Sauvegarde tous les 5 epochs
    'patience': 5,          # Early stopping
    'lr0': 0.01,           # Learning rate initial
    'warmup_epochs': 1,     # Epochs de warmup
    'verbose': True
}

print("Configuration d'entra√Ænement :")
for key, value in train_config.items():
    print(f"  {key}: {value}")
                </code></pre>
              </details>
              <table>
                  <thead>
                    <tr>
                      <th>Param√®tre</th>
                      <th>Description</th>
                      <th>Impact</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>data</td>
                      <td>Chemin vers le fichier YAML contenant les informations du dataset</td>
                    </tr>
                    <tr>
                      <td>epochs</td>
                      <td>Nombre total d'√©poques pour l'entra√Ænement(Nombre de fois que le mod√®le parcourt l'ensemble du dataset)</td>
                      <td>
                        <ul class="dash-list">
                          <li>Trop petit: le mod√®le peut ne pas apprendre correctement.</li>
                          <li>Trop grand: risque de sur-apprentissage, et entra√Ænement plus long.</li>
                        </ul>
                      </td>
                    </tr>
                    <tr>
                      <td>imgsz</td>
                      <td>Taille des images utilis√©es pour l'entra√Ænement</td>
                      <td>
                        <ul class="dash-list">
                          <li>Plus petit: rapide mais perte de d√©tails, pr√©cision r√©duite</li>
                          <li>Plus grand: meilleure pr√©cision possible, mais plus de m√©moire GPU n√©cessaire, entra√Ænement plus lent.</li>
                        </ul>
                      </td>
                    </tr>
                    <tr>
                      <td>batch</td>
                      <td>Nombre d'images trait√©es simultan√©ment</td>
                      <td>
                        <ul class="dash-list">
                          <li>Petit batch: moins de m√©moire n√©cessaire, mais entra√Ænement plus bruit√© et instable.</li>
                          <li>Grand batch: convergence plus stable, mais plus de m√©moire GPU n√©cessaire.</li>
                        </ul>
                      </td>
                    </tr>
                    <tr>
                      <td>device</td>
                      <td>Appareil utilis√© : GPU (0) si disponible, sinon CPU</td>
                      <td>
                        <ul class="dash-list">
                          <li>GPU: entra√Ænement beaucoup plus rapide, recommand√© si disponible.</li>
                          <li>CPU:  tr√®s lent, d√©conseill√© pour l‚Äôentra√Ænement de mod√®les complexes.</li>
                        </ul>
                      </td>
                    </tr>
                  </tbody>
              </table> 

            <h4>d. Entra√Ænement du mod√®le</h4>
              <p>
                Cette √©tape lance l'entra√Ænement du mod√®le YOLOv8 en utilisant la configuration d√©finie pr√©c√©demment. 
                Le processus ajuste les poids du mod√®le pour qu'il apprenne √† d√©tecter les objets du dataset.
              </p>
              <details class="code-container">
                <summary>
                  Entrainement du mod√®le
                </summary>
                <div class="btn-bar">
                  <button onclick="copierCode('train_model', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="train_model">
# Entra√Ænement du mod√®le
print("D√©but de l'entra√Ænement YOLOv8 !")
print("-" * 50)

# Lancement de l'entra√Ænement
results = model.train(**train_config)

print("Entra√Ænement termin√© !")
print(f"Mod√®le sauvegard√© dans : {results.save_dir}")
                </code></pre>
              </details>
              <ul>
                <li><strong>model.train(**train_config)</strong>: ex√©cute l'entra√Ænement avec tous les param√®tres d√©finis dans <strong><em>train_config</em></strong>.</li>
                <li><strong>results.save_dir</strong>: indique le dossier o√π le mod√®le entra√Æn√© et les r√©sultats sont sauvegard√©s.</li>
              </ul>
              <div class="image-container">
                <img src="../images/TRC25/YOLO/training_1.webp" alt="entrainement du modele" class="image-mean"/>
                <img src="../images/TRC25/YOLO/training_2.webp" alt="entrainement du modele" class="image-mean"/>
              </div>

            <h4>e. Visualisation des m√©triques</h4>
              <p>
                Apr√®s l'entra√Ænement, il est important de visualiser les m√©triques pour √©valuer les performances du mod√®le, comme la pr√©cision, le rappel, etc. 
                Cela permet de v√©rifier si le mod√®le apprend correctement et d'identifier d'√©ventuels probl√®mes.
              </p>
              <details class="code-container">
                <summary>
                  Visualisation des m√©triques
                </summary>
                <div class="btn-bar">
                  <button onclick="copierCode('metric_visualization', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="metric_visualization">
results_path = Path(f"{train_config["project"]}/{train_config["name"]}")
csv_path = results_path / 'results.csv'

if csv_path.exists():
    # Lecture des m√©triques
    df = pd.read_csv(csv_path)
    df.columns = df.columns.str.strip()

    # Suppression des colonnes inutiles
    if 'epoch' not in df.columns:
        print(" Colonne 'epoch' introuvable. V√©rifie ton fichier CSV.")
    else:
        metrics = [c for c in df.columns if c != 'epoch' and c !=
                  'time' and c != 'lr/pg0' and c != 'lr/pg1' and c != 'lr/pg2']

        n = len(metrics)
        cols = 5
        rows = math.ceil(n / cols)

        #Affichage de l'image sur jupyter notebook
        %matplotlib inline  
        fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows))
        axes = axes.flatten() if n > 1 else [axes]

        # Boucle sur chaque m√©trique
        for i, col in enumerate(metrics):
            axes[i].plot(df['epoch'], df[col], '-o', linewidth=2)
            axes[i].set_title(col, fontsize=10)
            axes[i].set_xlabel('epoch')
            axes[i].grid(True)

        # Si la grille a plus de cases que de m√©triques, on cache les cases vides
        for j in range(i + 1, len(axes)):
            axes[j].axis('off')

        plt.suptitle("√âvolution des m√©triques YOLO", fontsize=14)
        plt.tight_layout(rect=[0, 0, 1, 0.97])
        plt.show()

        # M√âTRIQUES FINALES
        final_metrics = df.iloc[-1]
        print("\n=== M√âTRIQUES FINALES ===")
        for col in metrics:
            val = final_metrics[col]
            if pd.api.types.is_numeric_dtype(type(val)):
                print(f"{col:30s} : {val:.4f}")
            else:
                print(f"{col:30s} : {val}")

else:
    print("Fichier de r√©sultats non trouv√© :", csv_path)
                </code></pre>
              </details>
              <div class="image-container">
                <img src="../images/TRC25/YOLO/metric_visualization.webp" alt="visualisation m√©trique" class="image-mean"/>
              </div>

              <table>
                  <thead>
                    <tr>
                      <th>M√©trique</th>
                      <th>Description</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>train/box_loss</td>
                      <td>Erreur sur la localisation des bo√Ætes englobantes pendant l‚Äôentra√Ænement</td>
                    </tr>
                    <tr>
                      <td>train/cls_loss</td>
                      <td>Erreur sur la classification des objets dans les bo√Ætes pendant l‚Äôentra√Ænement</td>
                    </tr>
                    <tr>
                      <td>train/dfl_loss</td>
                      <td>Erreur de distribution des distances pour affiner les bo√Ætes (Distance Focal Loss)</td>
                    </tr>
                    <tr>
                      <td>val/box_loss</td>
                      <td>Erreur sur la localisation des bo√Ætes sur le dataset de validation</td>
                    </tr>
                    <tr>
                      <td>val/cls_loss</td>
                      <td>Erreur sur la classification des objets sur le dataset de validation</td>
                    </tr>
                    <tr>
                      <td>val/dfl_loss</td>
                      <td>Erreur de distribution des distances sur le dataset de validation</td>
                    </tr>
                    <tr>
                      <td>metrics/precision(B)</td>
                      <td>Pr√©cision du mod√®le: proportion de d√©tections correctes sur toutes les d√©tections(batch)</td>
                    </tr>
                    <tr>
                      <td>metrics/recall(B)</td>
                      <td>Rappel: proportion d'objets r√©ellement pr√©sents correctement d√©tect√©s(batch)</td>
                    </tr>
                    <tr>
                      <td>metrics/mAP50(B)</td>
                      <td>Mean Average Precision √† IoU 0.5: qualit√© globale de d√©tection(batch)</td>
                    </tr>
                    <tr>
                      <td>metrics/mAP50-95(B)</td>
                      <td>Mean Average Precision moyenne sur IoU de 0.5 √† 0.95(batch)</td>
                    </tr>
                  </tbody>
              </table> 

            <h4>f. Evaluation sur donn√©es de validation</h4>
              <p>
                Une fois l'entra√Ænement termin√©, il est essentiel d'√©valuer le mod√®le sur le jeu de donn√©es de validation. 
                Cela permet de mesurer ses performances sur des donn√©es qu'il n'a jamais vues, et d'obtenir des m√©triques fiables 
                afin de v√©rifier que le mod√®le g√©n√©ralise correctement.
              </p>
              <details class="code-container">
                <summary>
                  Evaluation sur validation
                </summary>
                <div class="btn-bar">
                  <button onclick="copierCode('evaluation_val', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="evaluation_val">
# √âvaluation du mod√®le entra√Æn√©
print("√âvaluation du mod√®le...")

# Chargement du meilleur mod√®le
best_model_path = Path(f"{train_config["project"]}/{train_config["name"]}/weights/best.pt")

if os.path.exists(best_model_path):
    model = YOLO(best_model_path)
    print(f"Mod√®le charg√© : {best_model_path}")
else:
    print("Meilleur mod√®le non trouv√©, utilisation du mod√®le pr√©-entra√Æn√©")

# Validation
val_results = model.val(data='data.yaml', imgsz=imgsz)


precision = val_results.box.mp    # mean precision
recall = val_results.box.mr    # mean recall
f1_score = val_results.box.f1.mean()  # F1-score moyen sur toutes les classes
map50 = val_results.box.map50
map5095 = val_results.box.map

# Latence (pr√©process + inference + NMS)
latency = sum(val_results.speed.values())

# Affichage
print("\n=== R√©sultats d'√©valuation ===")
print(f"Precision : {precision:.3f}")
print(f"Recall    : {recall:.3f}")
print(f"F1-score  : {f1_score:.3f}")
print(f"mAP@0.5(Mean Average Precicion 0.5&lt;IOU)   : {map50:.3f}")
print(f"mAP@0.5:0.95(Mean Average Precicion 0.5&lt;IOU&lt;0.95) : {map5095:.3f}")
print(f"Latence moyenne : {latency:.3f} ms/image")
                </code></pre>
              </details>
              <div class="image-container">
                <img src="../images/TRC25/YOLO/evaluation_val.webp" alt="evaluation sur validation" class="image-mean"/>
              </div>

            <h4>g. Test du mod√®le</h4>
              <p>
                Apr√®s l'entra√Ænement et l'√©valuation, il est possible de tester le mod√®le sur une image individuelle pour visualiser 
                concr√®tement ses pr√©dictions et v√©rifier que les objets sont correctement d√©tect√©s
              </p>
              <h5>&#10022; Chargement du mod√®le et fonction de pr√©diction</h5>
              <details class="code-container">
                <summary>Chargement du mod√®le et fonction de pr√©diction</summary>
                <div class="btn-bar">
                  <button onclick="copierCode('predict_visualize', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="predict_visualize">
# Chargement du meilleur mod√®le
best_model_path = Path("runs/detect/train/weights/best.pt")

if os.path.exists(best_model_path):
    model = YOLO(best_model_path)
    print(f"Mod√®le charg√© : {best_model_path}")
else:
    print("Meilleur mod√®le non trouv√©, utilisation du mod√®le pr√©-entra√Æn√©")


def predict_and_visualize(model, image_path, conf_threshold=0.5):
    """
    Faire une pr√©diction et visualiser les r√©sultats
    """
    # Pr√©diction
    results = model(image_path, conf=conf_threshold)

    # Visualisation
    annotated_image = results[0].plot()

    # Conversion BGR -> RGB pour matplotlib
    annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)

    # Affichage
    #Affichage de l'image sur jupyter notebook
    %matplotlib inline     
    print("Affichage de l'image")
    plt.figure(figsize=(12, 8))
    plt.imshow(annotated_image)
    plt.axis('off')
    plt.title(f'D√©tections (Confiance > {conf_threshold})')
    plt.show()

    # Informations sur les d√©tections
    boxes = results[0].boxes
    if boxes is not None:
        print(f"{len(boxes)} objets d√©tect√©s :")
        for i, box in enumerate(boxes):
            class_id = int(box.cls[0])
            confidence = float(box.conf[0])
            class_name = model.names[class_id]
            print(f"  {i+1}. {class_name} (confiance: {confidence:.2f})")
    else:
        print("Aucun objet d√©tect√©")
                </code></pre>
              </details>

              <h5>&#10022; Test sur une image</h5>
              <details class="code-container">
                <summary>
                  Test sur une image
                </summary>
                <div class="btn-bar">
                  <button onclick="copierCode('test1_model', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="test1_model">
# Test sur une image d'exemple
print("Test sur image d'exemple...")

# Recherche d'images dans le dataset
image_files = glob.glob('dataset/images/*.jpg')
if image_files:
    test_image = image_files[0]
    print(f"Image de test : {test_image}")
    predict_and_visualize(model, test_image, conf_threshold=0.5)
else:
    print("Aucune image trouv√©e dans le dataset")
                </code></pre>
              </details>
              <div class="image-container">
                <img src="../images/TRC25/YOLO/test1_model.webp" alt="test du mod√®le sur une image" class="image-mean"/>
              </div>

              <h5>&#10022; Test sur plusieurs images dans un dossier</h5>
              <details class="code-container">
                <summary>
                  Tests sur un dossier d'images
                </summary>
                <div class="btn-bar">
                  <button onclick="copierCode('test2_model', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="test2_model">
import time
import keyboard 
# Parcours du dossier de test
test_root = Path("./test")

if not test_root.exists():
    print(f"Le dossier de test '{test_root}' n'existe pas.")
else:
    print("\n--- D√©but des pr√©dictions ---")
    stop_flag = False

    for img_path in test_root.glob("*"):
        if img_path.suffix.lower() in [".jpg", ".jpeg", ".png"]:
            # Pr√©diction + affichage
            predict_and_visualize(model, img_path, conf_threshold=0.5)

            # Pause interactive ou d√©lai 5s
            print("Appuyez sur 'Entr√©e' pour continuer, 'Esc' pour arr√™ter...")
            start_time = time.time()
            while True:
                if keyboard.is_pressed('esc'):
                    stop_flag = True
                    break
                if keyboard.is_pressed('enter') or (time.time() - start_time) > 1:
                    break
            if stop_flag:
                print("Arr√™t des pr√©dictions par l'utilisateur.")
                break
    print("\nPr√©dictions termin√©es.")
                </code></pre>
              </details>
              <div class="image-container">
                <img src="../images/TRC25/YOLO/test2_model1.webp" alt="test du mod√®le sur une image" class="image-small"/>
                <img src="../images/TRC25/YOLO/test2_model2.webp" alt="test du mod√®le sur une image" class="image-small"/>
                <img src="../images/TRC25/YOLO/test2_model3.webp" alt="test du mod√®le sur une image" class="image-small"/>
                <img src="../images/TRC25/YOLO/test2_model4.webp" alt="test du mod√®le sur une image" class="image-small"/>
                <img src="../images/TRC25/YOLO/test2_model5.webp" alt="test du mod√®le sur une image" class="image-small"/>
                <img src="../images/TRC25/YOLO/test2_model6.webp" alt="test du mod√®le sur une image" class="image-small"/>
              </div>

            <h4>h. Export du mod√®le</h4>
              <p>
                Une fois l'entra√Ænement termin√© et valid√©, il est important d'exporter le mod√®le dans diff√©rents formats 
                (par exemple <strong><em>.pt</em></strong>, <strong><em>.onnx</em></strong>, <strong><em>.tflite</em></strong>) afin de pouvoir l'utiliser pour l'inf√©rence dans d'autres environnements ou applications.
              </p>
              <details class="code-container">
                <summary>
                  Export du mod√®le
                </summary>
                <div class="btn-bar">
                  <button onclick="copierCode('export_model', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="export_model">
# Export du mod√®le pour d√©ploiement sur Jetson Nano
print("Export du mod√®le pour Jetson Nano...")

# 1. Export ONNX (format standard)
try:
    onnx_path = model.export(format='onnx', imgsz=640, optimize=True)
    print(f" Export ONNX r√©ussi : {onnx_path}")
except Exception as e:
    print(f" Erreur export ONNX : {e}")

# 2. Export TensorRT (sp√©cifique NVIDIA)
try:
    trt_path = model.export(format='engine', imgsz=640, half=True)
    print(f" Export TensorRT r√©ussi : {trt_path}")
except Exception as e:
    print(f" Export TensorRT non disponible (normal sur Colab) : {e}")

# 3. Export TorchScript (PyTorch optimis√©)
try:
    torchscript_path = model.export(format='torchscript', imgsz=640, optimize=True)
    print(f" Export TorchScript r√©ussi : {torchscript_path}")
except Exception as e:
    print(f" Erreur export TorchScript : {e}")

# 4. Sauvegarde du mod√®le PyTorch standard
model.save('b2ms_detector.pt')
print(" Mod√®le PyTorch sauvegard√© : b2ms_detector.pt")

print(f"\n Formats export√©s pour d√©ploiement :")
print(f"- PyTorch (.pt) : Usage g√©n√©ral")
print(f"- ONNX (.onnx) : Multi-plateforme")
print(f"- TensorRT (.engine) : Optimis√© Jetson Nano")
print(f"- TorchScript (.torchscript) : PyTorch optimis√©")
                </code></pre>
              </details>

            <p class="txt_center">
              <a href="https://github.com/TekBot-Robotics-Challenge/2025-Team-B2MS_CleanTech-Docs/raw/refs/heads/main/TRC25/b2ms_YOLO/B2MS_detection.ipynb" target="_blank" class="download-link">
                T√©l√©charger BMS_detection.ipynb
              </a>
            </p>

          <h3 id="5-yolo">5. Scripts python</h3>
            <p>
              Les codes sont regroup√©s sous forme de scripts Python pour simplifier leur utilisation. 
              Cela permet de configurer facilement les param√®tres selon les besoins, sans avoir √† modifier 
              directement les lignes de code, rendant ainsi le syst√®me plus flexible et accessible.
              Cette modularit√© est possible gr√¢ce √† <strong><em>argparse</em></strong>, qui permet de passer des options directement 
              en ligne de commande, avec des valeurs par d√©faut et une aide automatique pour l‚Äôutilisateur.
            </p>

            <h4>a. Environnement de travail</h4>
              <p>
                Il faut toujours commencer par activer/configurer son environnement virtuel et cr√©er un dossier de travail:
              </p>
              <div class="code-container">
                <div class="btn-bar">
                  <button onclick="copierCode('script_env', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="script_env">
conda activate b2ms-env
pip install -qU ultralytics opencv-python matplotlib pillow torch
pip install -qU --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124  #pour pc avec gpu
mkdir detection
cd detection
                </code></pre>
              </div>

    <p>
      Il faut ensuite copier le dossier <strong>scripts</strong> dans le repertoire de travail.
    </p>
            
            <h4>b. Pr√©paration du Dataset</h4>
              <div class="code-container">
                <div class="btn-bar">
                  <button onclick="copierCode('script_data', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="script_data">
python scripts/split_dataset.py --dataset_path="dataset" --train_per=0.8
                </code></pre>
              </div>
              
              <ul>
                <li>
                  <strong>--dataset_path</strong> : Chemin du dossier contenant les donn√©es (dataset/).
                </li>
                <li>
                  <strong>--train_per</strong> : Pourcentage du dataset utilis√© pour l'entra√Ænement (par d√©faut 0.8: 80%).
                </li>
              </ul>

            <h4>c. Entra√Ænement du mod√®le</h4>
              <div class="code-container">
                <div class="btn-bar">
                  <button onclick="copierCode('script_train', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="script_train">
python scripts/training_model.py --model=yolov8n.pt --epochs=20 --imgsz=640 --batch=16
                </code></pre>
              </div>
              <ul>
                <li>
                  <strong>--model</strong> : Mod√®le YOLO de d√©part (yolov8n.pt, yolov8s.pt, etc.).
                </li>
                <li>
                  <strong>--epochs</strong> : Nombre d'it√©rations d'apprentissage (d√©faut 20).
                </li>
                <li>
                  <strong>--imgsz</strong> : Taille des images d'entr√©e (d√©faut 640).
                </li>
                <li>
                  <strong>--batch</strong> : Taille du batch (d√©faut 16).
                </li>
              </ul>
              <p>
                Les r√©sultats d'entra√Ænement (modele entrain√©, logs, m√©triques) sont enregistr√©s dans : <strong><em>runs/detect/train/</em></strong>
              </p>
            <h4>d. Visualisation des m√©triques</h4>
              <div class="code-container">
                <div class="btn-bar">
                  <button onclick="copierCode('script_metric', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="script_metric">
python scripts/metric_visualization.py --result_path=runs/detect/train/results.csv
                </code></pre>
              </div>
              <ul>
                <li>
                  <strong>--result_path</strong> : Chemin du fichier results.csv g√©n√©r√© par l‚Äôentra√Ænement (par d√©faut runs/detect/train/results.csv).
                </li>
              </ul>

            <h4>e. Evaluation sur donn√©es de validation</h4>
              <div class="code-container">
                <div class="btn-bar">
                  <button onclick="copierCode('script_evaluation_val', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="script_evaluation_val">
python scripts/evaluate_val.py --model_path=runs/detect/train/weights/best.pt --imgsz=640
                </code></pre>
              </div>
              <ul>
                <li>
                  <strong>--model_path</strong> : Chemin du meilleur mod√®le (par d√©faut runs/detect/train/weights/best.pt).
                </li>
                <li>
                  <strong>--imgsz</strong> : Taille des images √† l‚Äô√©valuation (d√©faut 640).
                </li>
              </ul>

            <h4>f. Lancement du mod√®le</h4>
              <div class="code-container">
                <div class="btn-bar">
                  <button onclick="copierCode('script_launch', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="script_launch">
python scripts/detection_launch.py --model=runs/detect/train/weights/best.pt --source=usb0 --confidence=0.6 --resolution=640x640
                </code></pre>
              </div>
              <ul>
                <li>
                 <strong>--model</strong> : Chemin vers le mod√®le entra√Æn√©.
                </li>
                <li>
                  <strong>--source</strong> : Source des donn√©es :
                    <ul>
                      <li>image.jpg: une image unique
                      <li>data/images/: un dossier</li>
                      <li>video.mp4: une vid√©o</li>
                      <li>"usb0": cam√©ra USB</li></li>
                    </ul>
                  <li>
                    <strong>--confidence </strong>: Seuil minimal de confiance (d√©faut 0.5).
                  </li>
                  <li>
                    <strong>--resolution</strong> : R√©solution d‚Äôaffichage (ex. 1280x720).
                  </li>
                </li>
              </ul>

            <h4>g. Export du mod√®le</h4>
              <div class="code-container">
                <div class="btn-bar">
                  <button onclick="copierCode('script_export_model', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="script_export_model">
python scripts/export_model.py --model_path=runs/detect/train/weights/best.pt --export_to=models --name=b2ms_model --imgsz=640
                </code></pre>
              </div>
              <ul>
                <li>
                  <strong>--model_path</strong> : Chemin du mod√®le √† exporter (d√©faut : runs/detect/train/weights/best.pt).
                </li>
                <li>
                  <strong>--export_to</strong> : Dossier de destination pour les fichiers export√©s (d√©faut : models).
                </li>
                <li>
                  <strong>--name</strong> : Nom du mod√®le export√© (d√©faut : my_model).
                </li>
                <li>
                  <strong>--imgsz</strong> : Taille des images √† l'export (d√©faut 640).
                </li>
              </ul>

            <p class="txt_center">
              <a href="https://github.com/TekBot-Robotics-Challenge/2025-Team-B2MS_CleanTech-Docs/raw/refs/heads/main/TRC25/b2ms_YOLO/scripts_python.zip" target="_blank" class="download-link">
                T√©l√©charger scripts_python.zip
              </a>
            </p>

          <h3 id="6-yolo">6. YOLOv5 vs YOLOv8</h3>
            <h4>a. Comparaison num√©rique</h4>
              <h5>&#10022; Jeu de donn√©es COCO </h5>
                <p>
                  Pour commencer, les performances publi√©es par <a href="https://www.ultralytics.com/" target="_blank">Ultralytics</a> sont examin√©es. 
                  Les deux mod√®les, issus de la m√™me √©quipe, sont reconnus pour leur excellent compromis entre vitesse et pr√©cision. 
                  N√©anmoins, chacun d‚Äôeux est optimis√© pour r√©pondre √† des besoins diff√©rents en vision par ordinateur.
                </p>
                <div class="image-container">
                <img src="../images/TRC25/YOLO/comparaison_ultralytics.webp" alt="yolov5 vs yolov8 COCO" class="image-mean"/>
              </div>

              <p>Pour des tailles d'images de 640px:</p>
              <table>
                <thead>
                  <tr>
                    <th>Mod√®le</th>
                    <!-- <th>Taille (pixels)</th> -->
                    <th>mAPval 50-95</th>
                    <th>Vitesse CPU ONNX (ms)</th>
                    <th>Vitesse T4 TensorRT10 (ms)</th>
                    <th>Param√®tres (M)</th>
                    <th>FLOPs (B)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>YOLOv8n</td>
                    <!-- <td class="num">640</td> -->
                    <td class="num">37.3</td>
                    <td class="num">80.4</td>
                    <td class="num">1.47</td>
                    <td class="num">3.2</td>
                    <td class="num">8.7</td>
                  </tr>
                  <tr>
                    <td>YOLOv8s</td>
                    <!-- <td class="num">640</td> -->
                    <td class="num">44.9</td>
                    <td class="num">128.4</td>
                    <td class="num">2.66</td>
                    <td class="num">11.2</td>
                    <td class="num">28.6</td>
                  </tr>
                  <tr>
                    <td>YOLOv8m</td>
                    <!-- <td class="num">640</td> -->
                    <td class="num">50.2</td>
                    <td class="num">234.7</td>
                    <td class="num">5.86</td>
                    <td class="num">25.9</td>
                    <td class="num">78.9</td>
                  </tr>
                  <tr>
                    <td>YOLOv8l</td>
                    <!-- <td class="num">640</td> -->
                    <td class="num">52.9</td>
                    <td class="num">375.2</td>
                    <td class="num">9.06</td>
                    <td class="num">43.7</td>
                    <td class="num">165.2</td>
                  </tr>
                  <tr>
                    <td>YOLOv8x</td>
                    <!-- <td class="num">640</td> -->
                    <td class="num">53.9</td>
                    <td class="num">479.1</td>
                    <td class="num">14.37</td>
                    <td class="num">68.2</td>
                    <td class="num">257.8</td>
                  </tr>
                  <tr>
                    <td>YOLOv5n</td>
                    <!-- <td class="num">640</td> -->
                    <td class="num">28.0</td>
                    <td class="num">73.6</td>
                    <td class="num">1.12</td>
                    <td class="num">2.6</td>
                    <td class="num">7.7</td>
                  </tr>
                  <tr>
                    <td>YOLOv5s</td>
                    <!-- <td class="num">640</td> -->
                    <td class="num">37.4</td>
                    <td class="num">120.7</td>
                    <td class="num">1.92</td>
                    <td class="num">9.1</td>
                    <td class="num">24.0</td>
                  </tr>
                  <tr>
                    <td>YOLOv5m</td>
                    <!-- <td class="num">640</td> -->
                    <td class="num">45.4</td>
                    <td class="num">233.9</td>
                    <td class="num">4.03</td>
                    <td class="num">25.1</td>
                    <td class="num">64.2</td>
                  </tr>
                  <tr>
                    <td>YOLOv5l</td>
                    <!-- <td class="num">640</td> -->
                    <td class="num">49.0</td>
                    <td class="num">408.4</td>
                    <td class="num">6.61</td>
                    <td class="num">53.2</td>
                    <td class="num">135.0</td>
                  </tr>
                  <tr>
                    <td>YOLOv5x</td>
                    <!-- <td class="num">640</td> -->
                    <td class="num">50.7</td>
                    <td class="num">763.2</td>
                    <td class="num">11.89</td>
                    <td class="num">97.2</td>
                    <td class="num">246.4</td>
                  </tr>
                </tbody>
              </table>
              <ul>
                <li>
                  YOLOv5 est id√©al si la priorit√© est la vitesse, la stabilit√© et un √©cosyst√®me mature. 
                  Il reste tr√®s efficace pour les appareils √† ressources limit√©es et les projets qui n√©cessitent une solution √©prouv√©e.
                </li>
                <li>
                  YOLOv8 est plus pr√©cis, plus flexible et bas√© sur une architecture moderne sans ancres. 
                  C‚Äôest le meilleur choix pour les projets cherchant performance et polyvalence en production comme en recherche.
                </li>
              </ul>
              <p class="txt_center">
                <a href="https://docs.ultralytics.com/fr/compare/yolov8-vs-yolov5/#performance-face-off-yolov8-vs-yolov5" target="_blank">YOLOv5 vs. YOLOv8 : Une Comparaison D√©taill√©e</a>
              </p>

              <h5>&#10022; Le dataset personnel</h5>
                <p>
                  Les performances de YOLOv5nu et YOLOv8n sont ensuite √©valu√©es sur le dataset afin de d√©terminer lequel s‚Äôadapte le mieux aux donn√©es sp√©cifiques.
                </p>

              <p>Pour des tailles d'images de 640px:</p>
              <table>
                <thead>
                  <tr>
                    <th>Mod√®le</th>
                    <th>mAP@0.5 (%)</th>
                    <th>mAP@0.5:0.95 (%)</th>
                    <th>Precision (%)</th>
                    <th>Recall (%)</th>
                    <th>F1-score (%)</th>
                    <th>Latence (ms/image)</th>
                    <th>Param√®tres (M)</th>
                    <th>FLOPs (GFLOPs)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>YOLOv5nu</td>
                    <td class="num">94.1</td>
                    <td class="num">69.0</td>
                    <td class="num">87.6</td>
                    <td class="num">87.4</td>
                    <td class="num">87.4</td>
                    <td class="num">15.144</td>
                    <td class="num">2.50</td>
                    <td class="num">7.1</td>
                  </tr>
                  <tr>
                    <td>YOLOv8n</td>
                    <td class="num">96.4</td>
                    <td class="num">71.7</td>
                    <td class="num">92.0</td>
                    <td class="num">89.5</td>
                    <td class="num">90.7</td>
                    <td class="num">13.632</td>
                    <td class="num">3.01</td>
                    <td class="num">8.1</td>
                  </tr>
                </tbody>
              </table>
              <div class="image-container">
                <img src="../images/TRC25/YOLO/comparaison_yolov5nu_yolov8n.webp" alt="yolov5 vs yolov8 COCO" class="image-mean"/>
              </div>
              <ul>
                <li>
                  YOLOv5nu reste attractif pour sa simplicit√© et sa l√©g√®ret√©, mais il est l√©g√®rement moins performant en d√©tection.
                </li>
                <li>
                  YOLOv8n, en revanche, constitue le meilleur choix pour ce dataset, alliant pr√©cision et efficacit√©.
                </li>
              </ul>
              <p>
                il est important de noter que la diff√©rence entre les deux mod√®les n‚Äôest pas tr√®s grande : YOLOv8n apporte des am√©liorations, 
                mais YOLOv5nu reste tr√®s comp√©titif et offre des performances proches tout en √©tant aussi tr√®s rapide.
              </p>

            <h4>b. Comparaison sur un ordinateur</h4>
              <video controls width=auto>
                <source src="../videos/TRC25/YOLO/comparaison_yolov5_yolov8_pc1.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>
              <video controls width=auto>
                <source src="../videos/TRC25/YOLO/comparaison_yolov5_yolov8_pc2.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>
              <p>
                Les tests r√©alis√©s sur un ordinateur sous Windows √©quip√© d‚Äôun <strong>GPU</strong> montrent que 
                YOLOv8 et YOLOv5 pr√©sentent <strong>des performances globalement proches</strong>, 
                avec quelques erreurs observ√©es(non-d√©tection ou classification incorrecte).<br>
                En termes de vitesse, les r√©sultats restent √©lev√©s et relativement similaires : YOLOv8 oscille entre <strong>20.74 et 21.58 FPS</strong>, 
                tandis que YOLOv5 varie entre <strong>18.47 et 20.65 FPS</strong>.<br>
                Pour la pr√©cision, YOLOv8 se montre l√©g√®rement meilleur, m√™me si les deux mod√®les pr√©sentent parfois des erreurs similaires.<br>
                Dans l‚Äôensemble, sur cette machine, les deux mod√®les offrent des performances comparables, avec un l√©ger avantage pour YOLOv8 en pr√©cision et une vitesse globalement √©quivalente.
              </p>


            <h4>c. Comparaison sur une machine virtuelle similaire au Dofbot</h4>
              <p>
                Avant de tester le Dofbot directement, il est pr√©f√©rable de disposer d‚Äôun syst√®me simul√© aux caract√©ristiques proches.
                Ce syst√®me peut √™tre mis en place en installant Ubuntu sur une machine virtuelle.
              </p>
              <h5>&#10022; Cr√©ation et configuration du syst√®me sous Ubuntu 20.01: </h5>
                <table>
                  <thead>
                    <tr>
                      <th>Section</th>
                      <th>Configuration</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Nouvelle machine virtuelle</td>
                      <td>
                        <ul class="dash-list">
                          <li>Nom: Dofbot-Ubuntu</li>
                          <li>Type: Linux</li>
                          <li>Version: Ubuntu (64-bit)</li>
                        </ul>
                      </td>
                    </tr>
                    <tr>
                      <td>M√©moire vive (RAM)</td>
                      <td>4096MB (4GB)</td>
                    </tr>
                    <tr>
                      <td>Disque dur</td>
                      <td>
                        <ul class="dash-list">
                          <li>Type: VDI (VirtualBox Disk Image)</li>
                          <li>Stockage dynamique: activ√©</li>
                          <li>Taille: 30GB</li>
                        </ul>
                      </td>
                    </tr>
                    <tr>
                      <td>Processeur</td>
                      <td>
                        <ul class="dash-list">
                          <li>4 CPU</li>
                          <li>Activer PAE/N</li>
                        </ul>
                      </td>
                    </tr>
                    <tr>
                      <td>Carte graphique</td>
                      <td>
                        <ul class="dash-list">
                          <li>VRAM: 128MB</li>
                          <li>Activer l'acc√©l√©ration 3D</li>
                        </ul>
                      </td>
                    </tr>
                  </tbody>
                </table>
                <p>
                  Une fois le syst√®me d√©marr√©, il faut installer les outils requis avec les commandes suivantes :
                </p>

              <details class="code-container">
                <summary>
                  Installation des d√©pendances
                </summary>
                <div class="btn-bar">
                  <button onclick="copierCode('dofbot_ubuntu', this)">üìã</button>
                </div>
                <pre><code class="hljs" id="dofbot_ubuntu">
sudo apt update
sudo apt install git -y
sudo apt install python3-pip -y
pip3 install ultralytics
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
                </code></pre>
              </details>
              
              <p>
                <strong>NB:</strong> Pour pouvoir utiliser la camera sur une machine virtuelle: 
                <strong>P√©ripheriques>Webcams: activer USB camera</strong>.
              </p>
              <video controls width=auto>
                <source src="../videos/TRC25/YOLO/comparaison_yolov5_yolov8_sim1.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>
              <video controls width=auto>
                <source src="../videos/TRC25/YOLO/comparaison_yolov5_yolov8_sim2.mp4" type="video/mp4">
                Votre navigateur ne supporte pas la vid√©o HTML5.
              </video>
              <p>
                Sur l‚Äôensemble des tests r√©alis√©s, YOLOv8 se d√©marque par une meilleure qualit√© de d√©tection, bien que quelques erreurs subsistent. 
                YOLOv5 pr√©sente des performances de d√©tection l√©g√®rement inf√©rieures, avec des erreurs similaires √† celles observ√©es avec YOLOv8.<br>
                En termes de vitesse, YOLOv5 atteint en moyenne <strong>3.23 FPS</strong>, tandis que YOLOv8 se situe autour de <strong>2.70‚Äì2.74 FPS</strong>, ce qui confirme que YOLOv5 est l√©g√®rement plus rapide, mais moins pr√©cis que YOLOv8.
              </p>

              <p class="txt_center">
                <a href="https://github.com/TekBot-Robotics-Challenge/2025-Team-B2MS_CleanTech-Docs/raw/refs/heads/main/TRC25/b2ms_YOLO/b2ms_YOLO.zip" target="_blank" class="download-link">
                  T√©l√©charger b2ms_YOLO.zip
                </a>
              </p>

        </div>
      </section>
    </main>
  </div>

  <footer>
    Copyright ¬© 2025 Team B2MS CleanTech. All rights reserved.
  </footer>

  <script src="../scripts/script_commun.js"></script>
  <script src="../scripts/script_code.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</body>
</html>
